{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45837763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90574c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Hugging Face and temp\n",
    "import os, pathlib\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "\n",
    "# ✅ Step 1: Log in to Hugging Face\n",
    "login(\"hf\")\n",
    "\n",
    "dataset = load_dataset(\"multitask_textqa_benchmark\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "589253ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17cef16c3634608877a6d51f615f516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# ✅ Step 2: Define model name\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# ✅ Step 3: Load configuration\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.hidden_dropout_prob = 0.0\n",
    "config.attention_probs_dropout_prob = 0.0\n",
    "# config.num_labels = 2  # Uncomment if doing classification\n",
    "\n",
    "# ✅ Step 4: Load tokenizer with legacy=True to avoid conversion error\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    legacy=True  # Suppresses the warning/error with tokenizer.model\n",
    ")\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, torch_dtype=torch.bfloat16\n",
    "    \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "686b048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MJLoRAFA import apply_monkeyjump\n",
    "\n",
    "\n",
    "blocks_spec = {\n",
    "    #\"SiglipEncoderLayer\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25],\n",
    "    \"LlamaDecoderLayer\":  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31],\n",
    "}\n",
    "linears = [ \"up_proj\", \"down_proj\", \"v_proj\", \"o_proj\", \"gate_proj\"]  # add 'up_proj','down_proj', etc. \"out_proj\", \"fc1\", \"fc2\", \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "\n",
    "model = apply_monkeyjump(\n",
    "    model,\n",
    "    blocks=blocks_spec,\n",
    "    shared_expert=[\"up_proj\", \"down_proj\"],\n",
    "    linears=linears,\n",
    "    rank=2, alpha=5.0,\n",
    "    temperature=1.0,   # router T\n",
    "    ema_momentum=0.5,\n",
    "    top_k=1,\n",
    "    rep_mode=\"token\",\n",
    "    jitter_noise=0.1,\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d6bf0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preamble variants: [[128006, 78191, 128007, 271], [128006, 78191, 128007]]\n",
      "EOT ID: 128009, EOS ID: 128009\n",
      "Insert token ID: 128001\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GlueLlavaDataCollator:\n",
    "    \"\"\"\n",
    "    Text-only multitask collator for Llama 3 style processors on GLUE.\n",
    "    \"\"\"\n",
    "    tokenizer: Any\n",
    "    is_train: bool = True\n",
    "    pad_to_multiple_of: Optional[int] = 8\n",
    "    answer_prefix: str = \"The correct output is\"\n",
    "    debug: bool = False\n",
    "    force_left_padding: bool = True\n",
    "    insert_token_id: int = 128001  # Token to insert before eot_id\n",
    "\n",
    "    def __post_init__(self):\n",
    "        tok = self.tokenizer\n",
    "\n",
    "        if self.force_left_padding:\n",
    "            tok.padding_side = \"left\"\n",
    "\n",
    "        if tok.pad_token_id is None:\n",
    "            tok.pad_token = tok.eos_token\n",
    "\n",
    "        self.pad_id = tok.pad_token_id\n",
    "        self.eos_id = tok.eos_token_id\n",
    "\n",
    "        # Build preamble by getting actual special token IDs for Llama 3\n",
    "        try:\n",
    "            start_header_id = tok.convert_tokens_to_ids(\"<|start_header_id|>\")\n",
    "            end_header_id = tok.convert_tokens_to_ids(\"<|end_header_id|>\")\n",
    "            assistant_ids = tok.encode(\"assistant\", add_special_tokens=False)\n",
    "            newline_ids = tok.encode(\"\\n\\n\", add_special_tokens=False)\n",
    "            \n",
    "            base_preamble = [start_header_id] + assistant_ids + [end_header_id] + newline_ids\n",
    "            \n",
    "            self._preamble_variants = [\n",
    "                base_preamble,\n",
    "                [start_header_id] + assistant_ids + [end_header_id],\n",
    "            ]\n",
    "            \n",
    "            self.eot_id = tok.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            if self.debug:\n",
    "                print(f\"[warn] Could not build Llama 3 preamble: {e}\")\n",
    "            self._preamble_variants = []\n",
    "            self.eot_id = self.eos_id\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"Preamble variants: {self._preamble_variants}\")\n",
    "            print(f\"EOT ID: {self.eot_id}, EOS ID: {self.eos_id}\")\n",
    "            print(f\"Insert token ID: {self.insert_token_id}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _rfind_subseq(hay, needle) -> int:\n",
    "        if not needle or len(needle) > len(hay):\n",
    "            return -1\n",
    "        for s in range(len(hay) - len(needle), -1, -1):\n",
    "            if hay[s:s + len(needle)] == needle:\n",
    "                return s\n",
    "        return -1\n",
    "\n",
    "    def _find_assistant_start(self, ids) -> int:\n",
    "        for needle in self._preamble_variants:\n",
    "            pos = self._rfind_subseq(ids, needle)\n",
    "            if pos != -1:\n",
    "                return pos + len(needle)\n",
    "        return -1\n",
    "\n",
    "    def _first_eos_after(self, ids, start) -> int:\n",
    "        if start < 0:\n",
    "            return -1\n",
    "        for i in range(start, len(ids)):\n",
    "            if ids[i] == self.eot_id or ids[i] == self.eos_id:\n",
    "                return i\n",
    "        return len(ids)\n",
    "\n",
    "    def _insert_token_before_eot(\n",
    "        self, \n",
    "        input_ids: torch.Tensor, \n",
    "        attention_mask: Optional[torch.Tensor]\n",
    "    ) -> tuple:\n",
    "        \"\"\"\n",
    "        Insert self.insert_token_id before <|eot_id|> in each sequence.\n",
    "        Result: content<|end_of_text|><|eot_id|>\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        \n",
    "        if attention_mask is None:\n",
    "            attention_mask = (input_ids != self.pad_id).long()\n",
    "        \n",
    "        new_input_ids_list = []\n",
    "        new_attention_mask_list = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            ids = input_ids[i].tolist()\n",
    "            attn = attention_mask[i].tolist()\n",
    "            \n",
    "            # Find the last eot_id position in the valid (non-pad) region\n",
    "            eot_pos = -1\n",
    "            for j in range(len(ids) - 1, -1, -1):\n",
    "                if attn[j] == 1 and ids[j] == self.eot_id:\n",
    "                    eot_pos = j\n",
    "                    break\n",
    "            \n",
    "            if eot_pos != -1:\n",
    "                # Check if insert_token_id is already right before eot_id\n",
    "                if eot_pos > 0 and ids[eot_pos - 1] == self.insert_token_id:\n",
    "                    # Already in correct position\n",
    "                    new_input_ids_list.append(ids)\n",
    "                    new_attention_mask_list.append(attn)\n",
    "                else:\n",
    "                    # Insert token before eot_id\n",
    "                    new_ids = ids[:eot_pos] + [self.insert_token_id] + ids[eot_pos:]\n",
    "                    new_attn = attn[:eot_pos] + [1] + attn[eot_pos:]\n",
    "                    new_input_ids_list.append(new_ids)\n",
    "                    new_attention_mask_list.append(new_attn)\n",
    "            else:\n",
    "                # No eot_id found, append at end of valid tokens\n",
    "                last_valid_idx = -1\n",
    "                for j in range(len(ids) - 1, -1, -1):\n",
    "                    if attn[j] == 1:\n",
    "                        last_valid_idx = j\n",
    "                        break\n",
    "                \n",
    "                if last_valid_idx >= 0 and ids[last_valid_idx] != self.insert_token_id:\n",
    "                    ids.append(self.insert_token_id)\n",
    "                    attn.append(1)\n",
    "                \n",
    "                new_input_ids_list.append(ids)\n",
    "                new_attention_mask_list.append(attn)\n",
    "        \n",
    "        # Find max length and pad\n",
    "        max_len = max(len(ids) for ids in new_input_ids_list)\n",
    "        \n",
    "        if self.pad_to_multiple_of:\n",
    "            max_len = ((max_len + self.pad_to_multiple_of - 1) // self.pad_to_multiple_of) * self.pad_to_multiple_of\n",
    "        \n",
    "        # Left-pad all sequences to max_len\n",
    "        for i in range(batch_size):\n",
    "            pad_len = max_len - len(new_input_ids_list[i])\n",
    "            if pad_len > 0:\n",
    "                new_input_ids_list[i] = [self.pad_id] * pad_len + new_input_ids_list[i]\n",
    "                new_attention_mask_list[i] = [0] * pad_len + new_attention_mask_list[i]\n",
    "        \n",
    "        new_input_ids = torch.tensor(new_input_ids_list, dtype=input_ids.dtype)\n",
    "        new_attention_mask = torch.tensor(new_attention_mask_list, dtype=attention_mask.dtype)\n",
    "        \n",
    "        return new_input_ids, new_attention_mask\n",
    "\n",
    "    def __call__(self, examples: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        texts: List[str] = []\n",
    "        label_texts: List[str] = []\n",
    "\n",
    "        for ex in examples:\n",
    "            user_text = ex[\"question\"]\n",
    "            gold = ex.get(\"answer\", \"\")\n",
    "\n",
    "            if self.is_train:\n",
    "                assistant_text = f\"{self.answer_prefix} {gold}\"\n",
    "\n",
    "                conversation = [\n",
    "                    {\"role\": \"user\", \"content\": user_text},\n",
    "                    {\"role\": \"assistant\", \"content\": assistant_text},\n",
    "                ]\n",
    "\n",
    "                text = self.tokenizer.apply_chat_template(\n",
    "                    conversation,\n",
    "                    add_generation_prompt=False,\n",
    "                    tokenize=False,\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                conversation = [\n",
    "                    {\"role\": \"user\", \"content\": user_text},\n",
    "                ]\n",
    "\n",
    "                base = self.tokenizer.apply_chat_template(\n",
    "                    conversation,\n",
    "                    add_generation_prompt=True,\n",
    "                    tokenize=False,\n",
    "                )\n",
    "\n",
    "                text = base + self.answer_prefix\n",
    "\n",
    "            texts.append(text)\n",
    "            label_texts.append(gold)\n",
    "\n",
    "        batch = self.tokenizer(\n",
    "            text=texts,\n",
    "            padding=True,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attn = batch.get(\"attention_mask\", None)\n",
    "\n",
    "        # Insert token 128001 before <|eot_id|> in each sequence\n",
    "        input_ids, attn = self._insert_token_before_eot(input_ids, attn)\n",
    "        batch[\"input_ids\"] = input_ids\n",
    "        batch[\"attention_mask\"] = attn\n",
    "\n",
    "        labels = torch.full_like(input_ids, -100)\n",
    "\n",
    "        if self.is_train:\n",
    "            for i in range(input_ids.size(0)):\n",
    "                valid_pos = attn[i].nonzero(as_tuple=False).squeeze(-1)\n",
    "                compact_ids = input_ids[i, valid_pos].tolist()\n",
    "\n",
    "                start_c = self._find_assistant_start(compact_ids)\n",
    "                end_c = self._first_eos_after(compact_ids, start_c) if start_c != -1 else -1\n",
    "                end_c = end_c   # include EOS/eot_id in label span\n",
    "\n",
    "                if start_c != -1 and end_c > start_c:\n",
    "                    span_pos = valid_pos[start_c:end_c]\n",
    "                    labels[i, span_pos] = input_ids[i, span_pos]\n",
    "                elif self.debug:\n",
    "                    print(f\"[warn] assistant span not found for sample {i}\")\n",
    "                    print(f\"  compact_ids: {compact_ids[:50]}...\")\n",
    "                    print(f\"  looking for: {self._preamble_variants}\")\n",
    "        else:\n",
    "            batch[\"label_texts\"] = label_texts\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "train_collator = GlueLlavaDataCollator(tokenizer=tokenizer, is_train=True,  debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebf6b1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = train_collator(dataset['train'].select(range(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "324dd46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "To properly, thoroughly apply bug spray while camping,\n",
      "\n",
      "  Options: \n",
      " A. spray a heavy amount of the spray into the air, and then let the mist come down over you and coat you gently.\n",
      " B. stand with your arms wide apart and your mouth and eyes closed, while someone sprays you from top to bottom.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The correct output is B<|end_of_text|><|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(d['input_ids'][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d62eda2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!The correct output is B<|end_of_text|>!\n"
     ]
    }
   ],
   "source": [
    "k = []\n",
    "\n",
    "for i in d['labels'][2]:\n",
    "    if i == -100:\n",
    "        k.append(0)\n",
    "    else:\n",
    "        k.append(i)\n",
    "    \n",
    "print(tokenizer.decode(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "768e2356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'prompt', 'source', 'answer', 'category', 'dataset'],\n",
       "        num_rows: 70302\n",
       "    })\n",
       "    test_arc_challenge: Dataset({\n",
       "        features: ['question', 'prompt', 'source', 'answer', 'category', 'dataset'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test_arc_easy: Dataset({\n",
       "        features: ['question', 'prompt', 'source', 'answer', 'category', 'dataset'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test_boolq: Dataset({\n",
       "        features: ['question', 'prompt', 'source', 'answer', 'category', 'dataset'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_hellaswag: Dataset({\n",
       "        features: ['question', 'prompt', 'source', 'answer', 'category', 'dataset'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_openbookqa: Dataset({\n",
       "        features: ['question', 'prompt', 'source', 'answer', 'category', 'dataset'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test_piqa: Dataset({\n",
       "        features: ['question', 'prompt', 'source', 'answer', 'category', 'dataset'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_social_i_qa: Dataset({\n",
       "        features: ['question', 'prompt', 'source', 'answer', 'category', 'dataset'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_winogrande: Dataset({\n",
       "        features: ['question', 'prompt', 'source', 'answer', 'category', 'dataset'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73d43e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MonkeyTrainer] interval=1, stop_at=1000, update_on=micro, momentum=0.5\n"
     ]
    }
   ],
   "source": [
    "from MJtrainer import MonkeyTrainer\n",
    "from transformers import Trainer, TrainingArguments, TrainerCallback\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llava-lora-finetuned_our\",\n",
    "    per_device_train_batch_size=6,\n",
    "    gradient_accumulation_steps=2,  \n",
    "    save_total_limit=4,\n",
    "    save_steps=500000,\n",
    "    num_train_epochs=1,\n",
    "    remove_unused_columns=False, \n",
    "   \n",
    "    bf16=True,  \n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"no\",  \n",
    "    #eval_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "  \n",
    "    learning_rate=1e-3,\n",
    "    warmup_ratio=0.03,\n",
    "    weight_decay=0.00,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# moe_trainer.py\n",
    "import torch\n",
    "from transformers import Trainer\n",
    "\n",
    "\n",
    "# Example instantiation:\n",
    "trainer = MonkeyTrainer(\n",
    "    model=model,\n",
    "    args=training_args,                  # your HF TrainingArguments\n",
    "    train_dataset=dataset['train'],\n",
    "\n",
    "    data_collator=train_collator,         # your collator\n",
    "    momentum=0.5, \n",
    "\n",
    "\n",
    "    step_interval=1,                    # update every 10 train steps\n",
    "    stop_update_step=1000,               # stop updates at/after step 1000\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c2ba9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "[kmeans-init] Collecting representations (TOKEN-BASED)...\n",
      "============================================================\n",
      "[kmeans-init] Found 32 patched blocks, mode: TOKEN-BASED\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a549d9297ce54c09b7d86ce0b551e037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting representations:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "\n",
      "============================================================\n",
      "[kmeans-init] Initializing router centers...\n",
      "============================================================\n",
      "[kmeans-init] model.layers.0: E=3 (avg_sim=0.408)\n",
      "[kmeans-init] model.layers.1: E=3 (avg_sim=0.526)\n",
      "[kmeans-init] model.layers.2: E=3 (avg_sim=0.492)\n",
      "[kmeans-init] model.layers.3: E=3 (avg_sim=0.489)\n",
      "[kmeans-init] model.layers.4: E=3 (avg_sim=0.509)\n",
      "[kmeans-init] model.layers.5: E=3 (avg_sim=0.502)\n",
      "[kmeans-init] model.layers.6: E=3 (avg_sim=0.494)\n",
      "[kmeans-init] model.layers.7: E=3 (avg_sim=0.482)\n",
      "[kmeans-init] model.layers.8: E=3 (avg_sim=0.494)\n",
      "[kmeans-init] model.layers.9: E=3 (avg_sim=0.515)\n",
      "[kmeans-init] model.layers.10: E=3 (avg_sim=0.500)\n",
      "[kmeans-init] model.layers.11: E=3 (avg_sim=0.517)\n",
      "[kmeans-init] model.layers.12: E=3 (avg_sim=0.516)\n",
      "[kmeans-init] model.layers.13: E=3 (avg_sim=0.575)\n",
      "[kmeans-init] model.layers.14: E=3 (avg_sim=0.558)\n",
      "[kmeans-init] model.layers.15: E=3 (avg_sim=0.559)\n",
      "[kmeans-init] model.layers.16: E=3 (avg_sim=0.565)\n",
      "[kmeans-init] model.layers.17: E=3 (avg_sim=0.585)\n",
      "[kmeans-init] model.layers.18: E=3 (avg_sim=0.591)\n",
      "[kmeans-init] model.layers.19: E=3 (avg_sim=0.609)\n",
      "[kmeans-init] model.layers.20: E=3 (avg_sim=0.619)\n",
      "[kmeans-init] model.layers.21: E=3 (avg_sim=0.612)\n",
      "[kmeans-init] model.layers.22: E=3 (avg_sim=0.619)\n",
      "[kmeans-init] model.layers.23: E=3 (avg_sim=0.622)\n",
      "[kmeans-init] model.layers.24: E=3 (avg_sim=0.631)\n",
      "[kmeans-init] model.layers.25: E=3 (avg_sim=0.646)\n",
      "[kmeans-init] model.layers.26: E=3 (avg_sim=0.661)\n",
      "[kmeans-init] model.layers.27: E=3 (avg_sim=0.667)\n",
      "[kmeans-init] model.layers.28: E=3 (avg_sim=0.673)\n",
      "[kmeans-init] model.layers.29: E=3 (avg_sim=0.674)\n",
      "[kmeans-init] model.layers.30: E=3 (avg_sim=0.644)\n",
      "[kmeans-init] model.layers.31: E=3 (avg_sim=0.641)\n",
      "\n",
      "============================================================\n",
      " INITIALIZATION SUMMARY\n",
      "============================================================\n",
      " Mode: TOKEN (clustering tokens)\n",
      "------------------------------------------------------------\n",
      " Block                                      E_init   E_used   AvgSim\n",
      "------------------------------------------------------------\n",
      "  model.layers.0                                 3        3    0.408\n",
      "  model.layers.1                                 3        3    0.526\n",
      "  model.layers.2                                 3        3    0.492\n",
      "  model.layers.3                                 3        3    0.489\n",
      "  model.layers.4                                 3        3    0.509\n",
      "  model.layers.5                                 3        3    0.502\n",
      "  model.layers.6                                 3        3    0.494\n",
      "  model.layers.7                                 3        3    0.482\n",
      "  model.layers.8                                 3        3    0.494\n",
      "  model.layers.9                                 3        3    0.515\n",
      "  model.layers.10                                3        3    0.500\n",
      "  model.layers.11                                3        3    0.517\n",
      "  model.layers.12                                3        3    0.516\n",
      "  model.layers.13                                3        3    0.575\n",
      "  model.layers.14                                3        3    0.558\n",
      "  model.layers.15                                3        3    0.559\n",
      "  model.layers.16                                3        3    0.565\n",
      "  model.layers.17                                3        3    0.585\n",
      "  model.layers.18                                3        3    0.591\n",
      "  model.layers.19                                3        3    0.609\n",
      "  model.layers.20                                3        3    0.619\n",
      "  model.layers.21                                3        3    0.612\n",
      "  model.layers.22                                3        3    0.619\n",
      "  model.layers.23                                3        3    0.622\n",
      "  model.layers.24                                3        3    0.631\n",
      "  model.layers.25                                3        3    0.646\n",
      "  model.layers.26                                3        3    0.661\n",
      "  model.layers.27                                3        3    0.667\n",
      "  model.layers.28                                3        3    0.673\n",
      "  model.layers.29                                3        3    0.674\n",
      "  model.layers.30                                3        3    0.644\n",
      "  model.layers.31                                3        3    0.641\n",
      "============================================================\n",
      "[kmeans-init] Done!\n"
     ]
    }
   ],
   "source": [
    "from kmneas import init_router_centers\n",
    "init_router_centers(\n",
    "    trainer,\n",
    "    subset_size=5000,        # Use 2000 samples\n",
    "    loader_batch_size=8,\n",
    "    collect_batches=2000,\n",
    "    kmeans_iters=30,\n",
    "    seed=42,\n",
    "    verbose=True,\n",
    "  \n",
    "    # Optional: auto-select number of experts\n",
    "    auto_select_experts=False,\n",
    "    rep_mode=\"token\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84318271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5858' max='5858' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5858/5858 38:38, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.928200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.170200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.154200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.148900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.153700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.141000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.143500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.138400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.132700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.134500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.129200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.124500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.131600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.115300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.122100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.126100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.131700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.122200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.122500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.114900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.117000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.126000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.119400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.115700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.117600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.112300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.117600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.107100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.097700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.112500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.107800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.094700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.103900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.105600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.112400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.107600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.105600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.105300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.107800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.094000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.092400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.099500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.096700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.099100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.093600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.102800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.097600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.111500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.083400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.100600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.095200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.092100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.100300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.082500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.094300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.086100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5858, training_loss=0.19516952593768566, metrics={'train_runtime': 2319.2019, 'train_samples_per_second': 30.313, 'train_steps_per_second': 2.526, 'total_flos': 3.8978206671804826e+17, 'train_loss': 0.19516952593768566, 'epoch': 0.9999146539216524})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "931b6687",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_collator  = GlueLlavaDataCollator(tokenizer=tokenizer, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "342c2378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Winogrande Sample ===\n",
      "{'answer': 'B', 'question': 'Sarah was a much better surgeon than Maria so _ always got the easier cases.\\n\\n  Options: \\n A. Sarah\\n B. Maria', 'prompt': 'Choose the correct answer to fill in the blank.', 'source': '', 'category': 'WinoGrande', 'dataset': 'winogrande'}\n",
      "\n",
      "Answer: B\n",
      "\n",
      "Evaluating test_winogrande...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating commonsense: 100%|██████████| 40/40 [00:11<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[commonsense] accuracy: 1030/1267 = 0.8129\n",
      "preds[:10] ['B', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'A']\n",
      "golds[:10] ['B', 'A', 'B', 'A', 'A', 'A', 'A', 'B', 'B', 'A']\n",
      "\n",
      "Evaluating test_arc_challenge...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating commonsense: 100%|██████████| 37/37 [00:15<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[commonsense] accuracy: 917/1172 = 0.7824\n",
      "preds[:10] ['C', 'B', 'C', 'C', 'C', 'B', 'C', 'C', 'C', 'A']\n",
      "golds[:10] ['C', 'B', 'C', 'D', 'D', 'B', 'C', 'C', 'B', 'A']\n",
      "\n",
      "Evaluating test_arc_easy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating commonsense: 100%|██████████| 75/75 [00:29<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[commonsense] accuracy: 2114/2376 = 0.8897\n",
      "preds[:10] ['A', 'B', 'D', 'D', 'C', 'C', 'A', 'C', 'C', 'A']\n",
      "golds[:10] ['A', 'B', 'D', 'D', 'B', 'C', 'A', 'C', 'C', 'A']\n",
      "\n",
      "Evaluating test_boolq...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating commonsense: 100%|██████████| 103/103 [00:21<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[commonsense] accuracy: 2299/3270 = 0.7031\n",
      "preds[:10] ['A', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'A', 'A']\n",
      "golds[:10] ['B', 'A', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'A']\n",
      "\n",
      "Evaluating test_hellaswag...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating commonsense: 100%|██████████| 314/314 [04:44<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[commonsense] accuracy: 3680/10042 = 0.3665\n",
      "preds[:10] ['B', 'D', 'C', 'C', 'B', 'B', 'C', 'D', 'B', 'B']\n",
      "golds[:10] ['D', 'D', 'C', 'C', 'B', 'B', 'C', 'A', 'B', 'B']\n",
      "\n",
      "Evaluating test_openbookqa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating commonsense: 100%|██████████| 16/16 [00:04<00:00,  3.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[commonsense] accuracy: 411/500 = 0.8220\n",
      "preds[:10] ['B', 'A', 'C', 'C', 'C', 'C', 'C', 'B', 'B', 'B']\n",
      "golds[:10] ['B', 'A', 'C', 'C', 'C', 'C', 'C', 'B', 'D', 'B']\n",
      "\n",
      "Evaluating test_piqa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating commonsense: 100%|██████████| 58/58 [00:39<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[commonsense] accuracy: 1355/1838 = 0.7372\n",
      "preds[:10] ['B', 'B', 'B', 'A', 'A', 'B', 'B', 'A', 'A', 'B']\n",
      "golds[:10] ['A', 'B', 'B', 'B', 'A', 'B', 'B', 'A', 'A', 'A']\n",
      "\n",
      "Evaluating test_social_i_qa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating commonsense: 100%|██████████| 62/62 [00:19<00:00,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[commonsense] accuracy: 1402/1954 = 0.7175\n",
      "preds[:10] ['A', 'A', 'C', 'A', 'C', 'A', 'B', 'B', 'C', 'B']\n",
      "golds[:10] ['C', 'A', 'B', 'A', 'C', 'A', 'B', 'B', 'C', 'B']\n",
      "\n",
      "=== Commonsense Eval Results ===\n",
      "test_winogrande: 0.8129\n",
      "test_arc_challenge: 0.7824\n",
      "test_arc_easy: 0.8897\n",
      "test_boolq: 0.7031\n",
      "test_hellaswag: 0.3665\n",
      "test_openbookqa: 0.8220\n",
      "test_piqa: 0.7372\n",
      "test_social_i_qa: 0.7175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "LETTER = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "def make_mcq_pattern(prefix: str):\n",
    "    return re.compile(\n",
    "        rf\"({prefix}[1-9])\\s*:\\s*(.*?)(?=\\s+{prefix}[1-9]\\s*:|\\s+Answer format:|$)\",\n",
    "        flags=re.IGNORECASE | re.DOTALL\n",
    "    )\n",
    "\n",
    "PATTERNS = [\n",
    "    (make_mcq_pattern(\"Answer\"),   lambda k: k.lower()),   # Answer1..Answer4 (ARC/OpenBookQA/SocialIQA)\n",
    "    (make_mcq_pattern(\"Ending\"),   lambda k: k.lower()),   # Ending1..Ending4 (HellaSwag)\n",
    "    (make_mcq_pattern(\"Solution\"), lambda k: k.lower()),   # Solution1..Solution2 (PIQA)\n",
    "    (make_mcq_pattern(\"Option\"),   lambda k: k.lower()),   # Option1..Option2 (Winogrande)\n",
    "]\n",
    "\n",
    "def extract_question_text(instr: str) -> str:\n",
    "    m = re.search(r\"question:\\s*(.*?)(?:\\n\\s*\\n|$)\", instr, flags=re.I | re.S)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "\n",
    "    m2 = re.search(r\"to the question:\\s*(.*?)(?:\\n\\s*\\n|$)\", instr, flags=re.I | re.S)\n",
    "    if m2:\n",
    "        return m2.group(1).strip()\n",
    "\n",
    "    m3 = re.search(r\"sentence:\\s*(.*?)(?:\\n\\s*\\n|$)\", instr, flags=re.I | re.S)\n",
    "    if m3:\n",
    "        return m3.group(1).strip()\n",
    "\n",
    "    return instr.strip()\n",
    "\n",
    "def extract_options(instr: str):\n",
    "    for pat, norm_key in PATTERNS:\n",
    "        found = pat.findall(instr)\n",
    "        if found:\n",
    "            return [(norm_key(k), v.strip()) for k, v in found]\n",
    "\n",
    "    if re.search(r\"Answer format:\\s*true\\s*/\\s*false\", instr, flags=re.I):\n",
    "        return [(\"true\", \"true\"), (\"false\", \"false\")]\n",
    "\n",
    "    return []\n",
    "\n",
    "def build_answer_map(option_keys):\n",
    "    return {k: LETTER[i] for i, k in enumerate(option_keys)}\n",
    "\n",
    "def instruction_to_train_style(example, *, category: str, dataset_name: str):\n",
    "    instr = (example.get(\"instruction\") or \"\").strip()\n",
    "    gold = (example.get(\"answer\") or \"\").strip().lower()\n",
    "\n",
    "    # ---- Winogrande special case ----\n",
    "    if dataset_name == \"winogrande\":\n",
    "        # Extract options from instruction text: \"Option1: Sarah Option2: Maria\"\n",
    "        opt_match = re.search(r\"Option1:\\s*(\\S+)\\s+Option2:\\s*(\\S+)\", instr, re.IGNORECASE)\n",
    "        \n",
    "        if opt_match:\n",
    "            opt1 = opt_match.group(1)\n",
    "            opt2 = opt_match.group(2)\n",
    "            \n",
    "            # Extract question (sentence with blank) - text before \"Option1:\"\n",
    "            parts = re.split(r\"\\s*Option1:\", instr, flags=re.IGNORECASE)\n",
    "            q_text = parts[0].strip()\n",
    "            \n",
    "            # Clean up the question text - remove the prompt prefix\n",
    "            q_text = re.sub(r\"^Please choose the correct answer to fill in the blank to complete the given sentence:\\s*\", \"\", q_text, flags=re.I)\n",
    "            \n",
    "            question = (\n",
    "                f\"{q_text}\\n\\n\"\n",
    "                f\"  Options: \\n\"\n",
    "                f\" A. {opt1}\\n\"\n",
    "                f\" B. {opt2}\"\n",
    "            )\n",
    "            \n",
    "            # Map option1 -> A, option2 -> B\n",
    "            answer = {\"option1\": \"A\", \"option2\": \"B\"}.get(gold, gold)\n",
    "            \n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"prompt\": \"Choose the correct answer to fill in the blank.\",\n",
    "                \"source\": \"\",\n",
    "                \"answer\": answer,\n",
    "                \"category\": category,\n",
    "                \"dataset\": dataset_name,\n",
    "            }\n",
    "        \n",
    "        # Fallback if pattern doesn't match\n",
    "        return {\n",
    "            \"question\": extract_question_text(instr),\n",
    "            \"prompt\": \"Choose the correct answer to fill in the blank.\",\n",
    "            \"source\": \"\",\n",
    "            \"answer\": gold,\n",
    "            \"category\": category,\n",
    "            \"dataset\": dataset_name,\n",
    "        }\n",
    "\n",
    "    # ---- Default path (all other datasets) ----\n",
    "    q_text = extract_question_text(instr)\n",
    "    options = extract_options(instr)\n",
    "\n",
    "    if options:\n",
    "        option_keys = [k for k, _ in options]\n",
    "        option_texts = [t for _, t in options]\n",
    "        ans_map = build_answer_map(option_keys)\n",
    "\n",
    "        question = q_text + \"\\n\\n  Options: \\n\"\n",
    "        for i, text in enumerate(option_texts):\n",
    "            question += f\" {LETTER[i]}. {text}\\n\"\n",
    "        question = question.rstrip()\n",
    "\n",
    "        answer = ans_map.get(gold, gold)\n",
    "    else:\n",
    "        question = q_text\n",
    "        answer = gold\n",
    "\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"prompt\": \"Choose the correct answer to the question.\",\n",
    "        \"source\": \"\",\n",
    "        \"answer\": answer,\n",
    "        \"category\": category,\n",
    "        \"dataset\": dataset_name,\n",
    "    }\n",
    "\n",
    "def load_and_convert(url_or_path: str, *, category: str, dataset_name: str):\n",
    "    ds = load_dataset(\"json\", data_files=url_or_path, split=\"train\")\n",
    "    ds2 = ds.map(\n",
    "        instruction_to_train_style,\n",
    "        fn_kwargs={\"category\": category, \"dataset_name\": dataset_name},\n",
    "        remove_columns=ds.column_names,\n",
    "        desc=f\"Converting {dataset_name}\",\n",
    "    )\n",
    "    return ds2\n",
    "\n",
    "TEST_SOURCES = {\n",
    "    \"test_winogrande\": {\n",
    "        \"path\": \"https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/refs/heads/main/dataset/winogrande/test.json\",\n",
    "        \"category\": \"WinoGrande\",\n",
    "        \"dataset\": \"winogrande\",\n",
    "    },\n",
    "    \"test_arc_challenge\": {\n",
    "        \"path\": \"https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/refs/heads/main/dataset/ARC-Challenge/test.json\",\n",
    "        \"category\": \"ARC-Challenge\",\n",
    "        \"dataset\": \"arc_challenge\",\n",
    "    },\n",
    "    \"test_arc_easy\": {\n",
    "        \"path\": \"https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/refs/heads/main/dataset/ARC-Easy/test.json\",\n",
    "        \"category\": \"ARC-Easy\",\n",
    "        \"dataset\": \"arc_easy\",\n",
    "    },\n",
    "    \"test_boolq\": {\n",
    "        \"path\": \"https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/refs/heads/main/dataset/boolq/test.json\",\n",
    "        \"category\": \"BoolQ\",\n",
    "        \"dataset\": \"boolq\",\n",
    "    },\n",
    "    \"test_hellaswag\": {\n",
    "        \"path\": \"https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/refs/heads/main/dataset/hellaswag/test.json\",\n",
    "        \"category\": \"HellaSwag\",\n",
    "        \"dataset\": \"hellaswag\",\n",
    "    },\n",
    "    \"test_openbookqa\": {\n",
    "        \"path\": \"https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/refs/heads/main/dataset/openbookqa/test.json\",\n",
    "        \"category\": \"OpenBookQA\",\n",
    "        \"dataset\": \"openbookqa\",\n",
    "    },\n",
    "    \"test_piqa\": {\n",
    "        \"path\": \"https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/refs/heads/main/dataset/piqa/test.json\",\n",
    "        \"category\": \"PIQA\",\n",
    "        \"dataset\": \"piqa\",\n",
    "    },\n",
    "    \"test_social_i_qa\": {\n",
    "        \"path\": \"https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/refs/heads/main/dataset/social_i_qa/test.json\",\n",
    "        \"category\": \"SocialIQA\",\n",
    "        \"dataset\": \"social_i_qa\",\n",
    "    },\n",
    "}\n",
    "\n",
    "updated_tests = {\n",
    "    split: load_and_convert(meta[\"path\"], category=meta[\"category\"], dataset_name=meta[\"dataset\"])\n",
    "    for split, meta in TEST_SOURCES.items()\n",
    "}\n",
    "\n",
    "dataset = DatasetDict(updated_tests)\n",
    "\n",
    "# Verify Winogrande is processed correctly\n",
    "print(\"=== Winogrande Sample ===\")\n",
    "print(dataset[\"test_winogrande\"][0])\n",
    "print(f\"\\nAnswer: {dataset['test_winogrande'][0]['answer']}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def _extract_choice(pred: str):\n",
    "    \"\"\"\n",
    "    Extract a discrete choice from model text.\n",
    "    Supports:\n",
    "      - A/B/C/D\n",
    "      - answer1/answer2/answer3/answer4\n",
    "      - ending1..ending4\n",
    "      - solution1/solution2\n",
    "      - option1/option2 (Winogrande)\n",
    "      - true/false\n",
    "    Returns normalized label like \"A\"/\"B\"/\"C\"/\"D\" or \"true\"/\"false\" or \"\".\n",
    "    \"\"\"\n",
    "    if pred is None:\n",
    "        return \"\"\n",
    "\n",
    "    s = str(pred).strip().lower()\n",
    "\n",
    "    # common wrappers\n",
    "    # e.g. \"the correct output is B\", \"the correct answer is answer3\"\n",
    "    s = re.sub(r\"^the correct (output|answer) is\\s+\", \"\", s).strip()\n",
    "\n",
    "    # If it directly contains a letter choice, prefer first standalone A-D\n",
    "    m = re.search(r\"\\b([abcd])\\b\", s)\n",
    "    if m:\n",
    "        return m.group(1).upper()\n",
    "\n",
    "    # Map answer/ending/solution/option tokens to letters (1->A, 2->B, 3->C, 4->D)\n",
    "    m = re.search(r\"\\b(answer|ending|solution|option)\\s*([1-4])\\b\", s)\n",
    "    if m:\n",
    "        idx = int(m.group(2)) - 1\n",
    "        return \"ABCD\"[idx]\n",
    "\n",
    "    # BoolQ / yes-no style\n",
    "    if re.search(r\"\\btrue\\b\", s):\n",
    "        return \"true\"\n",
    "    if re.search(r\"\\bfalse\\b\", s):\n",
    "        return \"false\"\n",
    "\n",
    "    # fallback: first token stripped\n",
    "    tok = s.split()[0] if s else \"\"\n",
    "    tok = tok.strip(\" .,:;!?\")\n",
    "    return tok\n",
    "\n",
    "def evaluate_commonsense(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    dataset,\n",
    "    eval_collator,\n",
    "    batch_size: int = 8,\n",
    "    max_new_tokens: int = 10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluation for commonsense reasoning DatasetDict splits.\n",
    "    Metric: exact-match accuracy after normalization.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    golds = []\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, collate_fn=eval_collator)\n",
    "\n",
    "    for batch_examples in tqdm(loader, desc=\"Evaluating commonsense\"):\n",
    "        batch = {\n",
    "            k: v.to(model.device)\n",
    "            for k, v in batch_examples.items()\n",
    "            if isinstance(v, torch.Tensor)\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            gen = model.generate(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch.get(\"attention_mask\", None),\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        generated = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
    "\n",
    "        prefix = eval_collator.answer_prefix  # e.g. \"The correct output is\"\n",
    "\n",
    "        for full_output, gold in zip(generated, batch_examples[\"label_texts\"]):\n",
    "            idx = full_output.rfind(prefix)\n",
    "            if idx != -1:\n",
    "                pred_text = full_output[idx + len(prefix):].strip()\n",
    "            else:\n",
    "                pred_text = full_output.strip()\n",
    "\n",
    "            preds.append(pred_text)\n",
    "            golds.append(gold)\n",
    "\n",
    "    if len(golds) == 0:\n",
    "        raise ValueError(\"No gold labels collected; check that label_texts are in the batch.\")\n",
    "\n",
    "    norm_preds = [_extract_choice(p) for p in preds]\n",
    "    norm_golds = [_extract_choice(g) for g in golds]\n",
    "\n",
    "    correct = sum(p == g for p, g in zip(norm_preds, norm_golds))\n",
    "    acc = correct / len(norm_golds)\n",
    "\n",
    "    print(f\"[commonsense] accuracy: {correct}/{len(norm_golds)} = {acc:.4f}\")\n",
    "    print(\"preds[:10]\", norm_preds[:10])\n",
    "    print(\"golds[:10]\", norm_golds[:10])\n",
    "\n",
    "    return acc, preds, golds\n",
    "\n",
    "\n",
    "# Run evaluation\n",
    "results = {}\n",
    "\n",
    "for split_name in dataset.keys():\n",
    "    print(f\"\\nEvaluating {split_name}...\")\n",
    "    acc, preds, golds = evaluate_commonsense(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        dataset=dataset[split_name],\n",
    "        eval_collator=eval_collator,\n",
    "        batch_size=32,\n",
    "        max_new_tokens=10,\n",
    "    )\n",
    "    results[split_name] = acc\n",
    "\n",
    "print(\"\\n=== Commonsense Eval Results ===\")\n",
    "for split, acc in results.items():\n",
    "    print(f\"{split}: {acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ea947e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Winogrande Sample ===\n",
      "{'answer': 'B', 'question': 'Sarah was a much better surgeon than Maria so _ always got the easier cases.\\n\\n  Options: \\n A. Sarah\\n B. Maria', 'prompt': 'Choose the correct answer to fill in the blank.', 'source': '', 'category': 'WinoGrande', 'dataset': 'winogrande'}\n",
      "\n",
      "Answer: B\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "LETTER = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "def make_mcq_pattern(prefix: str):\n",
    "    return re.compile(\n",
    "        rf\"({prefix}[1-9])\\s*:\\s*(.*?)(?=\\s+{prefix}[1-9]\\s*:|\\s+Answer format:|$)\",\n",
    "        flags=re.IGNORECASE | re.DOTALL\n",
    "    )\n",
    "\n",
    "PATTERNS = [\n",
    "    (make_mcq_pattern(\"Answer\"),   lambda k: k.lower()),   # Answer1..Answer4 (ARC/OpenBookQA/SocialIQA)\n",
    "    (make_mcq_pattern(\"Ending\"),   lambda k: k.lower()),   # Ending1..Ending4 (HellaSwag)\n",
    "    (make_mcq_pattern(\"Solution\"), lambda k: k.lower()),   # Solution1..Solution2 (PIQA)\n",
    "    (make_mcq_pattern(\"Option\"),   lambda k: k.lower()),   # Option1..Option2 (Winogrande)\n",
    "]\n",
    "\n",
    "def extract_question_text(instr: str) -> str:\n",
    "    m = re.search(r\"question:\\s*(.*?)(?:\\n\\s*\\n|$)\", instr, flags=re.I | re.S)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "\n",
    "    m2 = re.search(r\"to the question:\\s*(.*?)(?:\\n\\s*\\n|$)\", instr, flags=re.I | re.S)\n",
    "    if m2:\n",
    "        return m2.group(1).strip()\n",
    "\n",
    "    m3 = re.search(r\"sentence:\\s*(.*?)(?:\\n\\s*\\n|$)\", instr, flags=re.I | re.S)\n",
    "    if m3:\n",
    "        return m3.group(1).strip()\n",
    "\n",
    "    return instr.strip()\n",
    "\n",
    "def extract_options(instr: str):\n",
    "    for pat, norm_key in PATTERNS:\n",
    "        found = pat.findall(instr)\n",
    "        if found:\n",
    "            return [(norm_key(k), v.strip()) for k, v in found]\n",
    "\n",
    "    if re.search(r\"Answer format:\\s*true\\s*/\\s*false\", instr, flags=re.I):\n",
    "        return [(\"true\", \"true\"), (\"false\", \"false\")]\n",
    "\n",
    "    return []\n",
    "\n",
    "def build_answer_map(option_keys):\n",
    "    return {k: LETTER[i] for i, k in enumerate(option_keys)}\n",
    "\n",
    "def instruction_to_train_style(example, *, category: str, dataset_name: str):\n",
    "    instr = (example.get(\"instruction\") or \"\").strip()\n",
    "    gold = (example.get(\"answer\") or \"\").strip().lower()\n",
    "\n",
    "    # ---- Winogrande special case ----\n",
    "    if dataset_name == \"winogrande\":\n",
    "        # Extract options from instruction text: \"Option1: Sarah Option2: Maria\"\n",
    "        opt_match = re.search(r\"Option1:\\s*(\\S+)\\s+Option2:\\s*(\\S+)\", instr, re.IGNORECASE)\n",
    "        \n",
    "        if opt_match:\n",
    "            opt1 = opt_match.group(1)\n",
    "            opt2 = opt_match.group(2)\n",
    "            \n",
    "            # Extract question (sentence with blank) - text before \"Option1:\"\n",
    "            parts = re.split(r\"\\s*Option1:\", instr, flags=re.IGNORECASE)\n",
    "            q_text = parts[0].strip()\n",
    "            \n",
    "            # Clean up the question text - remove the prompt prefix\n",
    "            q_text = re.sub(r\"^Please choose the correct answer to fill in the blank to complete the given sentence:\\s*\", \"\", q_text, flags=re.I)\n",
    "            \n",
    "            question = (\n",
    "                f\"{q_text}\\n\\n\"\n",
    "                f\"  Options: \\n\"\n",
    "                f\" A. {opt1}\\n\"\n",
    "                f\" B. {opt2}\"\n",
    "            )\n",
    "            \n",
    "            # Map option1 -> A, option2 -> B\n",
    "            answer = {\"option1\": \"A\", \"option2\": \"B\"}.get(gold, gold)\n",
    "            \n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"prompt\": \"Choose the correct answer to fill in the blank.\",\n",
    "                \"source\": \"\",\n",
    "                \"answer\": answer,\n",
    "                \"category\": category,\n",
    "                \"dataset\": dataset_name,\n",
    "            }\n",
    "        \n",
    "        # Fallback if pattern doesn't match\n",
    "        return {\n",
    "            \"question\": extract_question_text(instr),\n",
    "            \"prompt\": \"Choose the correct answer to fill in the blank.\",\n",
    "            \"source\": \"\",\n",
    "            \"answer\": gold,\n",
    "            \"category\": category,\n",
    "            \"dataset\": dataset_name,\n",
    "        }\n",
    "\n",
    "    # ---- Default path (all other datasets) ----\n",
    "    q_text = extract_question_text(instr)\n",
    "    options = extract_options(instr)\n",
    "\n",
    "    if options:\n",
    "        option_keys = [k for k, _ in options]\n",
    "        option_texts = [t for _, t in options]\n",
    "        ans_map = build_answer_map(option_keys)\n",
    "\n",
    "        question = q_text + \"\\n\\n  Options: \\n\"\n",
    "        for i, text in enumerate(option_texts):\n",
    "            question += f\" {LETTER[i]}. {text}\\n\"\n",
    "        question = question.rstrip()\n",
    "\n",
    "        answer = ans_map.get(gold, gold)\n",
    "    else:\n",
    "        question = q_text\n",
    "        answer = gold\n",
    "\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"prompt\": \"Choose the correct answer to the question.\",\n",
    "        \"source\": \"\",\n",
    "        \"answer\": answer,\n",
    "        \"category\": category,\n",
    "        \"dataset\": dataset_name,\n",
    "    }\n",
    "\n",
    "def load_and_convert(url_or_path: str, *, category: str, dataset_name: str):\n",
    "    ds = load_dataset(\"json\", data_files=url_or_path, split=\"train\")\n",
    "    ds2 = ds.map(\n",
    "        instruction_to_train_style,\n",
    "        fn_kwargs={\"category\": category, \"dataset_name\": dataset_name},\n",
    "        remove_columns=ds.column_names,\n",
    "        desc=f\"Converting {dataset_name}\",\n",
    "    )\n",
    "    return ds2\n",
    "\n",
    "TEST_SOURCES = {\n",
    "    \"test_winogrande\": {\n",
    "        \"path\": \"https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/refs/heads/main/dataset/winogrande/test.json\",\n",
    "        \"category\": \"WinoGrande\",\n",
    "        \"dataset\": \"winogrande\",\n",
    "    },\n",
    "    \"test_arc_challenge\": {\n",
    "        \"path\": \"https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/refs/heads/main/dataset/ARC-Challenge/test.json\",\n",
    "        \"category\": \"ARC-Challenge\",\n",
    "        \"dataset\": \"arc_challenge\",\n",
    "    },\n",
    "    \"test_arc_easy\": {\n",
    "        \"path\": \"https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/refs/heads/main/dataset/ARC-Easy/test.json\",\n",
    "        \"category\": \"ARC-Easy\",\n",
    "        \"dataset\": \"arc_easy\",\n",
    "    },\n",
    "    \"test_boolq\": {\n",
    "        \"path\": \"https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/refs/heads/main/dataset/boolq/test.json\",\n",
    "        \"category\": \"BoolQ\",\n",
    "        \"dataset\": \"boolq\",\n",
    "    },\n",
    "    \"test_hellaswag\": {\n",
    "        \"path\": \"https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/refs/heads/main/dataset/hellaswag/test.json\",\n",
    "        \"category\": \"HellaSwag\",\n",
    "        \"dataset\": \"hellaswag\",\n",
    "    },\n",
    "    \"test_openbookqa\": {\n",
    "        \"path\": \"https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/refs/heads/main/dataset/openbookqa/test.json\",\n",
    "        \"category\": \"OpenBookQA\",\n",
    "        \"dataset\": \"openbookqa\",\n",
    "    },\n",
    "    \"test_piqa\": {\n",
    "        \"path\": \"https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/refs/heads/main/dataset/piqa/test.json\",\n",
    "        \"category\": \"PIQA\",\n",
    "        \"dataset\": \"piqa\",\n",
    "    },\n",
    "    \"test_social_i_qa\": {\n",
    "        \"path\": \"https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/refs/heads/main/dataset/social_i_qa/test.json\",\n",
    "        \"category\": \"SocialIQA\",\n",
    "        \"dataset\": \"social_i_qa\",\n",
    "    },\n",
    "}\n",
    "\n",
    "updated_tests = {\n",
    "    split: load_and_convert(meta[\"path\"], category=meta[\"category\"], dataset_name=meta[\"dataset\"])\n",
    "    for split, meta in TEST_SOURCES.items()\n",
    "}\n",
    "\n",
    "dataset = DatasetDict(updated_tests)\n",
    "\n",
    "# Verify Winogrande is processed correctly\n",
    "print(\"=== Winogrande Sample ===\")\n",
    "print(dataset[\"test_winogrande\"][0])\n",
    "print(f\"\\nAnswer: {dataset['test_winogrande'][0]['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffcdd5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating test_winogrande...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating commonsense: 100%|██████████| 40/40 [00:11<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[commonsense] accuracy: 1019/1267 = 0.8043\n",
      "preds[:10] ['B', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'A']\n",
      "golds[:10] ['B', 'A', 'B', 'A', 'A', 'A', 'A', 'B', 'B', 'A']\n",
      "\n",
      "Evaluating test_arc_challenge...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating commonsense: 100%|██████████| 37/37 [00:15<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[commonsense] accuracy: 904/1172 = 0.7713\n",
      "preds[:10] ['C', 'B', 'C', 'C', 'C', 'B', 'C', 'C', 'C', 'A']\n",
      "golds[:10] ['C', 'B', 'C', 'D', 'D', 'B', 'C', 'C', 'B', 'A']\n",
      "\n",
      "Evaluating test_arc_easy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating commonsense: 100%|██████████| 75/75 [00:29<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[commonsense] accuracy: 2128/2376 = 0.8956\n",
      "preds[:10] ['A', 'B', 'D', 'D', 'C', 'C', 'A', 'C', 'C', 'A']\n",
      "golds[:10] ['A', 'B', 'D', 'D', 'B', 'C', 'A', 'C', 'C', 'A']\n",
      "\n",
      "Evaluating test_boolq...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating commonsense: 100%|██████████| 103/103 [00:21<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[commonsense] accuracy: 2310/3270 = 0.7064\n",
      "preds[:10] ['B', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'A', 'A']\n",
      "golds[:10] ['B', 'A', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'A']\n",
      "\n",
      "Evaluating test_hellaswag...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating commonsense: 100%|██████████| 314/314 [04:43<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[commonsense] accuracy: 3690/10042 = 0.3675\n",
      "preds[:10] ['B', 'D', 'D', 'C', 'B', 'B', 'C', 'A', 'B', 'D']\n",
      "golds[:10] ['D', 'D', 'C', 'C', 'B', 'B', 'C', 'A', 'B', 'B']\n",
      "\n",
      "Evaluating test_openbookqa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating commonsense: 100%|██████████| 16/16 [00:04<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[commonsense] accuracy: 419/500 = 0.8380\n",
      "preds[:10] ['B', 'A', 'C', 'C', 'C', 'C', 'C', 'B', 'D', 'B']\n",
      "golds[:10] ['B', 'A', 'C', 'C', 'C', 'C', 'C', 'B', 'D', 'B']\n",
      "\n",
      "Evaluating test_piqa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating commonsense: 100%|██████████| 58/58 [00:39<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[commonsense] accuracy: 1341/1838 = 0.7296\n",
      "preds[:10] ['A', 'B', 'B', 'A', 'B', 'B', 'B', 'A', 'B', 'B']\n",
      "golds[:10] ['A', 'B', 'B', 'B', 'A', 'B', 'B', 'A', 'A', 'A']\n",
      "\n",
      "Evaluating test_social_i_qa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating commonsense: 100%|██████████| 62/62 [00:18<00:00,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[commonsense] accuracy: 1389/1954 = 0.7108\n",
      "preds[:10] ['A', 'A', 'C', 'A', 'C', 'B', 'B', 'B', 'C', 'B']\n",
      "golds[:10] ['C', 'A', 'B', 'A', 'C', 'A', 'B', 'B', 'C', 'B']\n",
      "\n",
      "=== Commonsense Eval Results ===\n",
      "test_winogrande: 0.8043\n",
      "test_arc_challenge: 0.7713\n",
      "test_arc_easy: 0.8956\n",
      "test_boolq: 0.7064\n",
      "test_hellaswag: 0.3675\n",
      "test_openbookqa: 0.8380\n",
      "test_piqa: 0.7296\n",
      "test_social_i_qa: 0.7108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def _extract_choice(pred: str):\n",
    "    \"\"\"\n",
    "    Extract a discrete choice from model text.\n",
    "    Supports:\n",
    "      - A/B/C/D\n",
    "      - answer1/answer2/answer3/answer4\n",
    "      - ending1..ending4\n",
    "      - solution1/solution2\n",
    "      - option1/option2 (Winogrande)\n",
    "      - true/false\n",
    "    Returns normalized label like \"A\"/\"B\"/\"C\"/\"D\" or \"true\"/\"false\" or \"\".\n",
    "    \"\"\"\n",
    "    if pred is None:\n",
    "        return \"\"\n",
    "\n",
    "    s = str(pred).strip().lower()\n",
    "\n",
    "    # common wrappers\n",
    "    # e.g. \"the correct output is B\", \"the correct answer is answer3\"\n",
    "    s = re.sub(r\"^the correct (output|answer) is\\s+\", \"\", s).strip()\n",
    "\n",
    "    # If it directly contains a letter choice, prefer first standalone A-D\n",
    "    m = re.search(r\"\\b([abcd])\\b\", s)\n",
    "    if m:\n",
    "        return m.group(1).upper()\n",
    "\n",
    "    # Map answer/ending/solution/option tokens to letters (1->A, 2->B, 3->C, 4->D)\n",
    "    m = re.search(r\"\\b(answer|ending|solution|option)\\s*([1-4])\\b\", s)\n",
    "    if m:\n",
    "        idx = int(m.group(2)) - 1\n",
    "        return \"ABCD\"[idx]\n",
    "\n",
    "    # BoolQ / yes-no style\n",
    "    if re.search(r\"\\btrue\\b\", s):\n",
    "        return \"true\"\n",
    "    if re.search(r\"\\bfalse\\b\", s):\n",
    "        return \"false\"\n",
    "\n",
    "    # fallback: first token stripped\n",
    "    tok = s.split()[0] if s else \"\"\n",
    "    tok = tok.strip(\" .,:;!?\")\n",
    "    return tok\n",
    "\n",
    "def evaluate_commonsense(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    dataset,\n",
    "    eval_collator,\n",
    "    batch_size: int = 8,\n",
    "    max_new_tokens: int = 10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluation for commonsense reasoning DatasetDict splits.\n",
    "    Metric: exact-match accuracy after normalization.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    golds = []\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, collate_fn=eval_collator)\n",
    "\n",
    "    for batch_examples in tqdm(loader, desc=\"Evaluating commonsense\"):\n",
    "        batch = {\n",
    "            k: v.to(model.device)\n",
    "            for k, v in batch_examples.items()\n",
    "            if isinstance(v, torch.Tensor)\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            gen = model.generate(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch.get(\"attention_mask\", None),\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        generated = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
    "\n",
    "        prefix = eval_collator.answer_prefix  # e.g. \"The correct output is\"\n",
    "\n",
    "        for full_output, gold in zip(generated, batch_examples[\"label_texts\"]):\n",
    "            idx = full_output.rfind(prefix)\n",
    "            if idx != -1:\n",
    "                pred_text = full_output[idx + len(prefix):].strip()\n",
    "            else:\n",
    "                pred_text = full_output.strip()\n",
    "\n",
    "            preds.append(pred_text)\n",
    "            golds.append(gold)\n",
    "\n",
    "    if len(golds) == 0:\n",
    "        raise ValueError(\"No gold labels collected; check that label_texts are in the batch.\")\n",
    "\n",
    "    norm_preds = [_extract_choice(p) for p in preds]\n",
    "    norm_golds = [_extract_choice(g) for g in golds]\n",
    "\n",
    "    correct = sum(p == g for p, g in zip(norm_preds, norm_golds))\n",
    "    acc = correct / len(norm_golds)\n",
    "\n",
    "    print(f\"[commonsense] accuracy: {correct}/{len(norm_golds)} = {acc:.4f}\")\n",
    "    print(\"preds[:10]\", norm_preds[:10])\n",
    "    print(\"golds[:10]\", norm_golds[:10])\n",
    "\n",
    "    return acc, preds, golds\n",
    "\n",
    "\n",
    "# Run evaluation\n",
    "results = {}\n",
    "\n",
    "for split_name in dataset.keys():\n",
    "    print(f\"\\nEvaluating {split_name}...\")\n",
    "    acc, preds, golds = evaluate_commonsense(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        dataset=dataset[split_name],\n",
    "        eval_collator=eval_collator,\n",
    "        batch_size=32,\n",
    "        max_new_tokens=10,\n",
    "    )\n",
    "    results[split_name] = acc\n",
    "\n",
    "print(\"\\n=== Commonsense Eval Results ===\")\n",
    "for split, acc in results.items():\n",
    "    print(f\"{split}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b3921a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jan  5 14:18:48 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.105.08             Driver Version: 580.105.08     CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA H100 PCIe               Off |   00000000:17:00.0 Off |                    0 |\n",
      "| N/A   60C    P0            182W /  310W |   80101MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A         1544027      C   ...nda/envs/torch_env/bin/python      80092MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44aa773",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
