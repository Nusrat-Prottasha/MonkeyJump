{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7e941b-00a9-4719-88bb-51c45445770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets, DatasetDict, ClassLabel\n",
    "\n",
    "cache_dir = \"/hf_cache\"\n",
    "\n",
    "\n",
    "\n",
    "TASKS = [\"sst2\", \"qnli\", \"qqp\", \"cola\", \"mrpc\", \"stsb\"]\n",
    "MAX_PER_TASK = 5000\n",
    "\n",
    "def build_id2label(feature):\n",
    "    \"\"\"Return a dict mapping id -> label name, or None if regression.\"\"\"\n",
    "    if isinstance(feature, ClassLabel):\n",
    "        return {i: name for i, name in enumerate(feature.names)}\n",
    "    return None  # e.g. STS-B, regression (float scores)\n",
    "\n",
    "\n",
    "def make_preprocess_fn(task, id2label):\n",
    "    \"\"\"Create a preprocessing function that outputs a single 'text' field + string label.\"\"\"\n",
    "\n",
    "    def preprocess(example):\n",
    "        # ---- build the natural-language prompt + input text ----\n",
    "        if task == \"sst2\":\n",
    "            prompt = \"Task: Sentiment classification.\\n\" \\\n",
    "                     \"Decide whether the following sentence is positive or negative.\\n\"\n",
    "            body = f\"Sentence: {example['sentence']}\"\n",
    "        elif task == \"cola\":\n",
    "            prompt = \"Task: Grammatical acceptability.\\n\" \\\n",
    "                     \"Decide whether the following sentence is grammatically acceptable or unacceptable.\\n\"\n",
    "            body = f\"Sentence: {example['sentence']}\"\n",
    "        elif task == \"qnli\":\n",
    "            prompt = \"Task: Question-answer entailment.\\n\" \\\n",
    "                     \"Decide whether the sentence correctly answers the question (entailment or not_entailment).\\n\"\n",
    "            body = f\"Question: {example['question']}\\nSentence: {example['sentence']}\"\n",
    "        elif task == \"qqp\":\n",
    "            prompt = \"Task: Question paraphrase detection.\\n\" \\\n",
    "                     \"Decide whether the two questions are paraphrases of each other (duplicate or not_duplicate).\\n\"\n",
    "            body = f\"Sentence 1: {example['question1']}\\nSentence 2: {example['question2']}\"\n",
    "        elif task == \"mrpc\":\n",
    "            prompt = \"Task: Sentence paraphrase detection.\\n\" \\\n",
    "                     \"Decide whether the two sentences are paraphrases of each other (equivalent or not_equivalent).\\n\"\n",
    "            body = f\"Sentence 1: {example['sentence1']}\\nSentence 2: {example['sentence2']}\"\n",
    "        elif task == \"stsb\":\n",
    "            prompt = \"Task: Semantic textual similarity.\\n\" \\\n",
    "                     \"Rate the similarity of the two sentences on a scale from 0 (no meaning overlap) \" \\\n",
    "                     \"to 5 (equivalent in meaning).\\n\"\n",
    "            body = f\"Sentence 1: {example['sentence1']}\\nSentence 2: {example['sentence2']}\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown task: {task}\")\n",
    "\n",
    "        text = prompt + \"\\n\" + body\n",
    "\n",
    "        # ---- convert label to string label_text ----\n",
    "        raw_label = example[\"label\"]\n",
    "        if id2label is not None:\n",
    "            # classification: use GLUE label names (e.g. 'entailment', 'duplicate', etc.)\n",
    "            label_text = id2label[int(raw_label)]\n",
    "        else:\n",
    "            # STS-B regression: keep numeric label but as a string\n",
    "            # (you can bucket this into custom names if you prefer)\n",
    "            label_text = str(raw_label)\n",
    "\n",
    "        return {\n",
    "            \"text\": text,\n",
    "            \"label_text\": label_text,\n",
    "            \"task\": task,\n",
    "        }\n",
    "\n",
    "    return preprocess\n",
    "\n",
    "\n",
    "train_parts = []\n",
    "val_splits = {}\n",
    "\n",
    "for task in TASKS:\n",
    "    print(f\"Loading {task}...\")\n",
    "    raw = load_dataset(\"nyu-mll/glue\", task, cache_dir=cache_dir)\n",
    "\n",
    "    # label mapping (if classification)\n",
    "    id2label = build_id2label(raw[\"train\"].features[\"label\"])\n",
    "    preprocess = make_preprocess_fn(task, id2label)\n",
    "\n",
    "    # ---- TRAIN: sample up to 5k examples ----\n",
    "    train_ds = raw[\"train\"].shuffle(seed=42)\n",
    "    n = min(MAX_PER_TASK, len(train_ds))  # if <5k, just use all\n",
    "    train_ds = train_ds.select(range(n))\n",
    "    train_ds = train_ds.map(\n",
    "        preprocess,\n",
    "        remove_columns=train_ds.column_names,\n",
    "    )\n",
    "    train_parts.append(train_ds)\n",
    "\n",
    "    # ---- VALIDATION: full validation per task ----\n",
    "    val_ds = raw[\"validation\"].map(\n",
    "        preprocess,\n",
    "        remove_columns=raw[\"validation\"].column_names,\n",
    "    )\n",
    "    val_splits[f\"validation_{task}\"] = val_ds\n",
    "\n",
    "# ---- FINAL MERGED DATASETDICT ----\n",
    "combined_train = concatenate_datasets(train_parts)\n",
    "\n",
    "multi_glue = DatasetDict({\n",
    "    \"train\": combined_train,       # one big train set\n",
    "    **val_splits,                 # validation_sst2, validation_qnli, ...\n",
    "})\n",
    "\n",
    "print(multi_glue)\n",
    "print(multi_glue[\"train\"][0][\"text\"])\n",
    "print(multi_glue[\"train\"][0][\"label_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ff9626a-4d6c-4356-b96a-b9af8e39165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_glue[\"train\"] = multi_glue[\"train\"].shuffle(seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fe9daac-eaf9-4ec6-9c18-5c8bf52239ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    LlavaOnevisionForConditionalGeneration,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    PreTrainedTokenizerBase\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_from_disk\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48731515-0c10-4571-89a4-fdbdc6c46860",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:06<00:00,  1.53s/it]\n"
     ]
    }
   ],
   "source": [
    "# ðŸš€ Step 2: Load Model and Processor\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForCausalLM, DataCollatorForLanguageModeling\n",
    "\n",
    "cache_dir = \"/hf_cache\"\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    legacy=True  # Suppresses the warning/error with tokenizer.model\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\", cache_dir=cache_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf442773-00a5-48c0-860d-161b83ff64de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b11e41f4-ee66-4ede-a089-225f3958a4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MJLoRA import apply_monkeyjump\n",
    "\n",
    "\n",
    "\n",
    "blocks_spec = {\n",
    "    #\"SiglipEncoderLayer\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25],\n",
    "    \"LlamaDecoderLayer\":  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31],\n",
    "}\n",
    "linears = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\"]  # add 'up_proj','down_proj', etc. \"out_proj\", \"fc1\", \"fc2\", \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "\n",
    "model = apply_monkeyjump(\n",
    "    model,\n",
    "    blocks=blocks_spec,\n",
    "    shared_expert=[\"gate_proj\",  \"o_proj\"],\n",
    "    linears=linears,\n",
    "    rank=2, alpha=3.0,\n",
    "    temperature=1.0,   # router T\n",
    "    ema_momentum=0.5,\n",
    "    top_k=1,\n",
    "    rep_mode=\"prompt_end\",\n",
    "    jitter_noise=0.01,\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "941812a2-b057-4051-a896-7e172f9809e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): MonkeyJumpLinear[expert_0](in=4096, out=4096, rank=2, dtype=float32)\n",
       "          (k_proj): MonkeyJumpLinear[expert_1](in=4096, out=1024, rank=2, dtype=float32)\n",
       "          (v_proj): MonkeyJumpLinear[expert_2](in=4096, out=1024, rank=2, dtype=float32)\n",
       "          (o_proj): MonkeyJumpLinear[shared](in=4096, out=4096, rank=2, dtype=float32)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): MonkeyJumpLinear[shared](in=4096, out=14336, rank=2, dtype=float32)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4a3d536-9c36-432d-aa1b-caa65ad82477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters:2883584\n"
     ]
    }
   ],
   "source": [
    "# Count of trainable parameters\n",
    "total_trainable_params = 0\n",
    "total =  0\n",
    "# Print trainable parameters and count their total number\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        #print(f\"Parameter name: {name}, Shape: {param.shape}\")\n",
    "        \n",
    "        total_trainable_params += param.numel()\n",
    "    total+=param.numel()\n",
    "\n",
    "print(f\"Total trainable parameters:{total_trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ddf9c0e-aba3-4457-8071-da9bc8c05ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,883,584 / 8,033,144,832\n"
     ]
    }
   ],
   "source": [
    "# 1) Count params that require grad\n",
    "trainable = [(n, p) for n, p in model.named_parameters() if p.requires_grad]\n",
    "total = sum(p.numel() for _, p in model.named_parameters())\n",
    "trainable_num = sum(p.numel() for _, p in trainable)\n",
    "print(f\"trainable params: {trainable_num:,} / {total:,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "389af6c1-7e89-4cbf-bf14-06a480a45354",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GlueLlavaDataCollator:\n",
    "    \"\"\"\n",
    "    Text-only multitask collator for Llama 3 style processors on GLUE.\n",
    "    \"\"\"\n",
    "    tokenizer: Any\n",
    "    is_train: bool = True\n",
    "    pad_to_multiple_of: Optional[int] = 8\n",
    "    answer_prefix: str = \"The correct output is\"\n",
    "    debug: bool = False\n",
    "    force_left_padding: bool = True\n",
    "    insert_token_id: int = 128001  # Token to insert before eot_id\n",
    "\n",
    "    def __post_init__(self):\n",
    "        tok = self.tokenizer\n",
    "\n",
    "        if self.force_left_padding:\n",
    "            tok.padding_side = \"left\"\n",
    "\n",
    "        if tok.pad_token_id is None:\n",
    "            tok.pad_token = tok.eos_token\n",
    "\n",
    "        self.pad_id = tok.pad_token_id\n",
    "        self.eos_id = tok.eos_token_id\n",
    "\n",
    "        # Build preamble by getting actual special token IDs for Llama 3\n",
    "        try:\n",
    "            start_header_id = tok.convert_tokens_to_ids(\"<|start_header_id|>\")\n",
    "            end_header_id = tok.convert_tokens_to_ids(\"<|end_header_id|>\")\n",
    "            assistant_ids = tok.encode(\"assistant\", add_special_tokens=False)\n",
    "            newline_ids = tok.encode(\"\\n\\n\", add_special_tokens=False)\n",
    "            \n",
    "            base_preamble = [start_header_id] + assistant_ids + [end_header_id] + newline_ids\n",
    "            \n",
    "            self._preamble_variants = [\n",
    "                base_preamble,\n",
    "                [start_header_id] + assistant_ids + [end_header_id],\n",
    "            ]\n",
    "            \n",
    "            self.eot_id = tok.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            if self.debug:\n",
    "                print(f\"[warn] Could not build Llama 3 preamble: {e}\")\n",
    "            self._preamble_variants = []\n",
    "            self.eot_id = self.eos_id\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"Preamble variants: {self._preamble_variants}\")\n",
    "            print(f\"EOT ID: {self.eot_id}, EOS ID: {self.eos_id}\")\n",
    "            print(f\"Insert token ID: {self.insert_token_id}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _rfind_subseq(hay, needle) -> int:\n",
    "        if not needle or len(needle) > len(hay):\n",
    "            return -1\n",
    "        for s in range(len(hay) - len(needle), -1, -1):\n",
    "            if hay[s:s + len(needle)] == needle:\n",
    "                return s\n",
    "        return -1\n",
    "\n",
    "    def _find_assistant_start(self, ids) -> int:\n",
    "        for needle in self._preamble_variants:\n",
    "            pos = self._rfind_subseq(ids, needle)\n",
    "            if pos != -1:\n",
    "                return pos + len(needle)\n",
    "        return -1\n",
    "\n",
    "    def _first_eos_after(self, ids, start) -> int:\n",
    "        if start < 0:\n",
    "            return -1\n",
    "        for i in range(start, len(ids)):\n",
    "            if ids[i] == self.eot_id or ids[i] == self.eos_id:\n",
    "                return i\n",
    "        return len(ids)\n",
    "\n",
    "    def _insert_token_before_eot(\n",
    "        self, \n",
    "        input_ids: torch.Tensor, \n",
    "        attention_mask: Optional[torch.Tensor]\n",
    "    ) -> tuple:\n",
    "        \"\"\"\n",
    "        Insert self.insert_token_id before <|eot_id|> in each sequence.\n",
    "        Result: content<|end_of_text|><|eot_id|>\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        \n",
    "        if attention_mask is None:\n",
    "            attention_mask = (input_ids != self.pad_id).long()\n",
    "        \n",
    "        new_input_ids_list = []\n",
    "        new_attention_mask_list = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            ids = input_ids[i].tolist()\n",
    "            attn = attention_mask[i].tolist()\n",
    "            \n",
    "            # Find the last eot_id position in the valid (non-pad) region\n",
    "            eot_pos = -1\n",
    "            for j in range(len(ids) - 1, -1, -1):\n",
    "                if attn[j] == 1 and ids[j] == self.eot_id:\n",
    "                    eot_pos = j\n",
    "                    break\n",
    "            \n",
    "            if eot_pos != -1:\n",
    "                # Check if insert_token_id is already right before eot_id\n",
    "                if eot_pos > 0 and ids[eot_pos - 1] == self.insert_token_id:\n",
    "                    # Already in correct position\n",
    "                    new_input_ids_list.append(ids)\n",
    "                    new_attention_mask_list.append(attn)\n",
    "                else:\n",
    "                    # Insert token before eot_id\n",
    "                    new_ids = ids[:eot_pos] + [self.insert_token_id] + ids[eot_pos:]\n",
    "                    new_attn = attn[:eot_pos] + [1] + attn[eot_pos:]\n",
    "                    new_input_ids_list.append(new_ids)\n",
    "                    new_attention_mask_list.append(new_attn)\n",
    "            else:\n",
    "                # No eot_id found, append at end of valid tokens\n",
    "                last_valid_idx = -1\n",
    "                for j in range(len(ids) - 1, -1, -1):\n",
    "                    if attn[j] == 1:\n",
    "                        last_valid_idx = j\n",
    "                        break\n",
    "                \n",
    "                if last_valid_idx >= 0 and ids[last_valid_idx] != self.insert_token_id:\n",
    "                    ids.append(self.insert_token_id)\n",
    "                    attn.append(1)\n",
    "                \n",
    "                new_input_ids_list.append(ids)\n",
    "                new_attention_mask_list.append(attn)\n",
    "        \n",
    "        # Find max length and pad\n",
    "        max_len = max(len(ids) for ids in new_input_ids_list)\n",
    "        \n",
    "        if self.pad_to_multiple_of:\n",
    "            max_len = ((max_len + self.pad_to_multiple_of - 1) // self.pad_to_multiple_of) * self.pad_to_multiple_of\n",
    "        \n",
    "        # Left-pad all sequences to max_len\n",
    "        for i in range(batch_size):\n",
    "            pad_len = max_len - len(new_input_ids_list[i])\n",
    "            if pad_len > 0:\n",
    "                new_input_ids_list[i] = [self.pad_id] * pad_len + new_input_ids_list[i]\n",
    "                new_attention_mask_list[i] = [0] * pad_len + new_attention_mask_list[i]\n",
    "        \n",
    "        new_input_ids = torch.tensor(new_input_ids_list, dtype=input_ids.dtype)\n",
    "        new_attention_mask = torch.tensor(new_attention_mask_list, dtype=attention_mask.dtype)\n",
    "        \n",
    "        return new_input_ids, new_attention_mask\n",
    "\n",
    "    def __call__(self, examples: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        texts: List[str] = []\n",
    "        label_texts: List[str] = []\n",
    "\n",
    "        for ex in examples:\n",
    "            user_text = ex[\"text\"]\n",
    "            gold = ex.get(\"label_text\", \"\")\n",
    "\n",
    "            if self.is_train:\n",
    "                assistant_text = f\"{self.answer_prefix} {gold}\"\n",
    "\n",
    "                conversation = [\n",
    "                    {\"role\": \"user\", \"content\": user_text},\n",
    "                    {\"role\": \"assistant\", \"content\": assistant_text},\n",
    "                ]\n",
    "\n",
    "                text = self.tokenizer.apply_chat_template(\n",
    "                    conversation,\n",
    "                    add_generation_prompt=False,\n",
    "                    tokenize=False,\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                conversation = [\n",
    "                    {\"role\": \"user\", \"content\": user_text},\n",
    "                ]\n",
    "\n",
    "                base = self.tokenizer.apply_chat_template(\n",
    "                    conversation,\n",
    "                    add_generation_prompt=True,\n",
    "                    tokenize=False,\n",
    "                )\n",
    "\n",
    "                text = base + self.answer_prefix\n",
    "\n",
    "            texts.append(text)\n",
    "            label_texts.append(gold)\n",
    "\n",
    "        batch = self.tokenizer(\n",
    "            text=texts,\n",
    "            padding=True,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attn = batch.get(\"attention_mask\", None)\n",
    "\n",
    "        # Insert token 128001 before <|eot_id|> in each sequence\n",
    "        input_ids, attn = self._insert_token_before_eot(input_ids, attn)\n",
    "        batch[\"input_ids\"] = input_ids\n",
    "        batch[\"attention_mask\"] = attn\n",
    "\n",
    "        labels = torch.full_like(input_ids, -100)\n",
    "\n",
    "        if self.is_train:\n",
    "            for i in range(input_ids.size(0)):\n",
    "                valid_pos = attn[i].nonzero(as_tuple=False).squeeze(-1)\n",
    "                compact_ids = input_ids[i, valid_pos].tolist()\n",
    "\n",
    "                start_c = self._find_assistant_start(compact_ids)\n",
    "                end_c = self._first_eos_after(compact_ids, start_c) if start_c != -1 else -1\n",
    "                end_c = end_c   # include EOS/eot_id in label span\n",
    "\n",
    "                if start_c != -1 and end_c > start_c:\n",
    "                    span_pos = valid_pos[start_c:end_c]\n",
    "                    labels[i, span_pos] = input_ids[i, span_pos]\n",
    "                elif self.debug:\n",
    "                    print(f\"[warn] assistant span not found for sample {i}\")\n",
    "                    print(f\"  compact_ids: {compact_ids[:50]}...\")\n",
    "                    print(f\"  looking for: {self._preamble_variants}\")\n",
    "        else:\n",
    "            batch[\"label_texts\"] = label_texts\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "49e0a052-5ca8-4504-ad4f-ee9e3d1e3d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preamble variants: [[128006, 78191, 128007, 271], [128006, 78191, 128007]]\n",
      "EOT ID: 128009, EOS ID: 128009\n",
      "Insert token ID: 128001\n"
     ]
    }
   ],
   "source": [
    "train_collator = GlueLlavaDataCollator(tokenizer=tokenizer, is_train=True,  debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ba42a000-d953-4703-8041-e34ff7dad547",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = train_collator(multi_glue['train'].select(range(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d3ad51b1-11a2-4cbf-9cc1-2e741643dbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Task: Grammatical acceptability.\n",
      "Decide whether the following sentence is grammatically acceptable or unacceptable.\n",
      "\n",
      "Sentence: Frances hid Sally of the presents.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The correct output is unacceptable<|end_of_text|><|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(d['input_ids'][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "11f2e472-71ee-4e1f-b857-7f6c0e7c4455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!The correct output is unacceptable<|end_of_text|>!\n"
     ]
    }
   ],
   "source": [
    "k = []\n",
    "\n",
    "for i in d['labels'][2]:\n",
    "    if i == -100:\n",
    "        k.append(0)\n",
    "    else:\n",
    "        k.append(i)\n",
    "    \n",
    "print(tokenizer.decode(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "19f4f911-5f58-4eeb-a763-235a5a28f724",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MonkeyTrainer] interval=2, stop_at=10000, update_on=micro, momentum=0.5\n"
     ]
    }
   ],
   "source": [
    "from MJtrainer import MonkeyTrainer\n",
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llava-lora-finetuned_our\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,  \n",
    "    save_total_limit=3,\n",
    "    save_steps=500000,\n",
    "    num_train_epochs=1,\n",
    "    remove_unused_columns=False, \n",
    "   \n",
    "    bf16=True,  \n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"no\",  \n",
    "    #eval_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "  \n",
    "    learning_rate=1e-3,\n",
    "    warmup_ratio=0.03,\n",
    "    weight_decay=0.00,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import Trainer, TrainingArguments, TrainerCallback\n",
    "\n",
    "\n",
    "\n",
    "# moe_trainer.py\n",
    "import torch\n",
    "from transformers import Trainer\n",
    "\n",
    "\n",
    "# Example instantiation:\n",
    "trainer = MonkeyTrainer(\n",
    "    model=model,\n",
    "    args=training_args,                  # your HF TrainingArguments\n",
    "    train_dataset=multi_glue['train'],\n",
    "\n",
    "    data_collator=train_collator,         # your collator\n",
    "    momentum=0.5,\n",
    "\n",
    "\n",
    "    step_interval=2,                    # update every 10 train steps\n",
    "    stop_update_step=10000,               # stop updates at/after step 1000\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#original 16:58\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "01e2fb6b-cc3c-4fbc-ab7a-3fcc97ec3485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "[kmeans-init] Collecting representations (SEQUENCE-BASED (prompt_lengths))...\n",
      "============================================================\n",
      "[kmeans-init] Found 32 patched blocks, mode: SEQUENCE-BASED (prompt_lengths)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting representations:   3%|â–Ž         | 625/20000 [01:00<31:01, 10.41it/s, samples=160000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[kmeans-init] Block collected 5000 sentences, dim=4096\n",
      "[kmeans-init] Block collected 5000 sentences, dim=4096\n",
      "[kmeans-init] Block collected 5000 sentences, dim=4096\n",
      "[kmeans-init] Block collected 5000 sentences, dim=4096\n",
      "[kmeans-init] Block collected 5000 sentences, dim=4096\n",
      "[kmeans-init] Block collected 5000 sentences, dim=4096\n",
      "[kmeans-init] Block collected 5000 sentences, dim=4096\n",
      "[kmeans-init] Block collected 5000 sentences, dim=4096\n",
      "[kmeans-init] Block collected 5000 sentences, dim=4096\n",
      "[kmeans-init] Block collected 5000 sentences, dim=4096\n",
      "[kmeans-init] Block collected 5000 sentences, dim=4096\n",
      "[kmeans-init] Block collected 5000 sentences, dim=4096\n",
      "[kmeans-init] Block collected 5000 sentences, dim=4096\n",
      "[kmeans-init] Block collected 5000 sentences, dim=4096\n",
      "[kmeans-init] Block collected 5000 sentences, dim=4096\n",
      "[kmeans-init] Block collected 5000 sentences, dim=4096\n",
      "[kmeans-init] Block collected 5000 sentences, dim=4096\n",
      "[kmeans-init] Block collected 5000 sentences, dim=4096\n",
      "[kmeans-init] Block collected 5000 sentences, dim=4096\n",
      "[kmeans-init] Block collected 5000 sentences, dim=4096\n",
      "[kmeans-init] Block collected 5000 sentences, dim=4096\n",
      "[kmeans-init] Block collected 5000 sentences, dim=4096\n",
      "[kmeans-init] Block collected 5000 sentences, dim=4096\n",
      "[kmeans-init] Block collected 5000 sentences, dim=4096\n",
      "[kmeans-init] Block collected 5000 sentences, dim=4096\n",
      "[kmeans-init] Block collected 5000 sentences, dim=4096\n",
      "[kmeans-init] Block collected 5000 sentences, dim=4096\n",
      "[kmeans-init] Block collected 5000 sentences, dim=4096\n",
      "[kmeans-init] Block collected 5000 sentences, dim=4096\n",
      "[kmeans-init] Block collected 5000 sentences, dim=4096\n",
      "[kmeans-init] Block collected 5000 sentences, dim=4096\n",
      "[kmeans-init] Block collected 5000 sentences, dim=4096\n",
      "\n",
      "============================================================\n",
      "[kmeans-init] Initializing router centers...\n",
      "============================================================\n",
      "[kmeans-init] model.layers.0: E=3 (avg_sim=0.940)\n",
      "[kmeans-init] model.layers.1: E=3 (avg_sim=0.982)\n",
      "[kmeans-init] model.layers.2: E=3 (avg_sim=1.000)\n",
      "[kmeans-init] model.layers.3: E=3 (avg_sim=1.000)\n",
      "[kmeans-init] model.layers.4: E=3 (avg_sim=0.999)\n",
      "[kmeans-init] model.layers.5: E=3 (avg_sim=0.999)\n",
      "[kmeans-init] model.layers.6: E=3 (avg_sim=0.998)\n",
      "[kmeans-init] model.layers.7: E=3 (avg_sim=0.998)\n",
      "[kmeans-init] model.layers.8: E=3 (avg_sim=0.997)\n",
      "[kmeans-init] model.layers.9: E=3 (avg_sim=0.997)\n",
      "[kmeans-init] model.layers.10: E=3 (avg_sim=0.996)\n",
      "[kmeans-init] model.layers.11: E=3 (avg_sim=0.996)\n",
      "[kmeans-init] model.layers.12: E=3 (avg_sim=0.996)\n",
      "[kmeans-init] model.layers.13: E=3 (avg_sim=0.995)\n",
      "[kmeans-init] model.layers.14: E=3 (avg_sim=0.994)\n",
      "[kmeans-init] model.layers.15: E=3 (avg_sim=0.994)\n",
      "[kmeans-init] model.layers.16: E=3 (avg_sim=0.992)\n",
      "[kmeans-init] model.layers.17: E=3 (avg_sim=0.991)\n",
      "[kmeans-init] model.layers.18: E=3 (avg_sim=0.988)\n",
      "[kmeans-init] model.layers.19: E=3 (avg_sim=0.987)\n",
      "[kmeans-init] model.layers.20: E=3 (avg_sim=0.987)\n",
      "[kmeans-init] model.layers.21: E=3 (avg_sim=0.985)\n",
      "[kmeans-init] model.layers.22: E=3 (avg_sim=0.982)\n",
      "[kmeans-init] model.layers.23: E=3 (avg_sim=0.981)\n",
      "[kmeans-init] model.layers.24: E=3 (avg_sim=0.980)\n",
      "[kmeans-init] model.layers.25: E=3 (avg_sim=0.979)\n",
      "[kmeans-init] model.layers.26: E=3 (avg_sim=0.979)\n",
      "[kmeans-init] model.layers.27: E=3 (avg_sim=0.979)\n",
      "[kmeans-init] model.layers.28: E=3 (avg_sim=0.980)\n",
      "[kmeans-init] model.layers.29: E=3 (avg_sim=0.979)\n",
      "[kmeans-init] model.layers.30: E=3 (avg_sim=0.977)\n",
      "[kmeans-init] model.layers.31: E=3 (avg_sim=0.973)\n",
      "\n",
      "============================================================\n",
      " INITIALIZATION SUMMARY\n",
      "============================================================\n",
      " Mode: PROMPT_LENGTHS (clustering sentences)\n",
      "------------------------------------------------------------\n",
      " Block                                      E_init   E_used   AvgSim\n",
      "------------------------------------------------------------\n",
      "  model.layers.0                                 3        3    0.940\n",
      "  model.layers.1                                 3        3    0.982\n",
      "  model.layers.2                                 3        3    1.000\n",
      "  model.layers.3                                 3        3    1.000\n",
      "  model.layers.4                                 3        3    0.999\n",
      "  model.layers.5                                 3        3    0.999\n",
      "  model.layers.6                                 3        3    0.998\n",
      "  model.layers.7                                 3        3    0.998\n",
      "  model.layers.8                                 3        3    0.997\n",
      "  model.layers.9                                 3        3    0.997\n",
      "  model.layers.10                                3        3    0.996\n",
      "  model.layers.11                                3        3    0.996\n",
      "  model.layers.12                                3        3    0.996\n",
      "  model.layers.13                                3        3    0.995\n",
      "  model.layers.14                                3        3    0.994\n",
      "  model.layers.15                                3        3    0.994\n",
      "  model.layers.16                                3        3    0.992\n",
      "  model.layers.17                                3        3    0.991\n",
      "  model.layers.18                                3        3    0.988\n",
      "  model.layers.19                                3        3    0.987\n",
      "  model.layers.20                                3        3    0.987\n",
      "  model.layers.21                                3        3    0.985\n",
      "  model.layers.22                                3        3    0.982\n",
      "  model.layers.23                                3        3    0.981\n",
      "  model.layers.24                                3        3    0.980\n",
      "  model.layers.25                                3        3    0.979\n",
      "  model.layers.26                                3        3    0.979\n",
      "  model.layers.27                                3        3    0.979\n",
      "  model.layers.28                                3        3    0.980\n",
      "  model.layers.29                                3        3    0.979\n",
      "  model.layers.30                                3        3    0.977\n",
      "  model.layers.31                                3        3    0.973\n",
      "============================================================\n",
      "[kmeans-init] Done!\n"
     ]
    }
   ],
   "source": [
    "from kmneas import init_router_centers\n",
    "init_router_centers(\n",
    "    trainer,\n",
    "    subset_size=5000,        # Use 2000 samples\n",
    "    loader_batch_size=8,\n",
    "    collect_batches=20000,\n",
    "    per_block_cap=50000,      \n",
    "    max_tokens_per_batch=40096,\n",
    "    kmeans_iters=50,\n",
    "    seed=42,\n",
    "    verbose=True,\n",
    "    # Optional: auto-select number of experts\n",
    "    auto_select_experts=False,\n",
    "    rep_mode=\"prompt_lengths\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9a641984-5ae3-4454-a382-c53c3cab40b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1792' max='1792' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1792/1792 07:53, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.158000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.323500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.295700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.247100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.255200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.221600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.220400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.228800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.209300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.220400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.201600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.206400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.195600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.198600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.201600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.185800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.171400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1792, training_loss=0.33006735678230015, metrics={'train_runtime': 473.8942, 'train_samples_per_second': 60.495, 'train_steps_per_second': 3.781, 'total_flos': 1.6021254393534874e+17, 'train_loss': 0.33006735678230015, 'epoch': 1.0})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "13894486-264c-442b-ae42-92188391e671",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_collator  = GlueLlavaDataCollator(tokenizer=tokenizer, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e7eecd6a-c4d6-4bd5-ba59-60e09293cd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating validation_stsb...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating stsb: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47/47 [00:23<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STS-B] Pearson: 0.9091 (n=1500)\n",
      "preds[:10] ['5.0', '4.199999809265137', '4.400000095367432', '2.200000047683716', '2.200000047683716', '2.5999999046325684', '5.0', '3.200000047683716', '4.0', '4.800000190734863']\n",
      "golds[:10] ['5.0', '4.75', '5.0', '2.4000000953674316', '2.75', '2.615000009536743', '5.0', '2.3329999446868896', '3.75', '5.0']\n",
      "\n",
      "Evaluating validation_sst2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating sst2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:04<00:00,  6.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sst2] accuracy: 832/872 = 0.9541\n",
      "preds[:10] ['positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative']\n",
      "golds[:10] ['positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative']\n",
      "\n",
      "Evaluating validation_qnli...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating qnli: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 171/171 [01:01<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[qnli] accuracy: 5058/5463 = 0.9259\n",
      "preds[:10] ['entailment', 'not_entailment', 'entailment', 'entailment', 'not_entailment', 'not_entailment', 'not_entailment', 'not_entailment', 'not_entailment', 'entailment']\n",
      "golds[:10] ['entailment', 'not_entailment', 'not_entailment', 'entailment', 'not_entailment', 'not_entailment', 'not_entailment', 'not_entailment', 'not_entailment', 'entailment']\n",
      "\n",
      "Evaluating validation_qqp...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating qqp: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1264/1264 [05:07<00:00,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[qqp] accuracy: 34337/40430 = 0.8493\n",
      "preds[:10] ['not_duplicate', 'not_duplicate', 'duplicate', 'not_duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate', 'not_duplicate']\n",
      "golds[:10] ['not_duplicate', 'not_duplicate', 'duplicate', 'not_duplicate', 'not_duplicate', 'duplicate', 'duplicate', 'duplicate', 'not_duplicate', 'not_duplicate']\n",
      "\n",
      "Evaluating validation_cola...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating cola: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:04<00:00,  7.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cola] accuracy: 870/1043 = 0.8341\n",
      "preds[:10] ['acceptable', 'acceptable', 'acceptable', 'acceptable', 'acceptable', 'acceptable', 'unacceptable', 'unacceptable', 'acceptable', 'acceptable']\n",
      "golds[:10] ['acceptable', 'acceptable', 'acceptable', 'acceptable', 'unacceptable', 'unacceptable', 'unacceptable', 'acceptable', 'acceptable', 'acceptable']\n",
      "\n",
      "Evaluating validation_mrpc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating mrpc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:03<00:00,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mrpc] accuracy: 360/408 = 0.8824\n",
      "preds[:10] ['equivalent', 'not_equivalent', 'not_equivalent', 'equivalent', 'not_equivalent', 'equivalent', 'not_equivalent', 'equivalent', 'equivalent', 'equivalent']\n",
      "golds[:10] ['equivalent', 'not_equivalent', 'not_equivalent', 'equivalent', 'not_equivalent', 'equivalent', 'not_equivalent', 'equivalent', 'equivalent', 'equivalent']\n",
      "\n",
      "=== GLUE Eval Results ===\n",
      "stsb: 0.9091\n",
      "sst2: 0.9541\n",
      "qnli: 0.9259\n",
      "qqp: 0.8493\n",
      "cola: 0.8341\n",
      "mrpc: 0.8824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr  # Add this import\n",
    "\n",
    "def _extract_first_number(text: str):\n",
    "    \"\"\"\n",
    "    Extract the first float-looking number from a string.\n",
    "    Returns float or None.\n",
    "    \"\"\"\n",
    "    m = re.search(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\", text)\n",
    "    if m:\n",
    "        try:\n",
    "            return float(m.group(0))\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def evaluate_glue(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    dataset,\n",
    "    eval_collator,\n",
    "    task: str,\n",
    "    batch_size: int = 8,\n",
    "    max_new_tokens: int = 10,\n",
    "):\n",
    "    \"\"\"\n",
    "    task: one of [\"sst2\", \"qnli\", \"qqp\", \"cola\", \"mrpc\", \"stsb\"].\n",
    "    For STS-B we compute Pearson correlation.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    golds = []\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, collate_fn=eval_collator)\n",
    "\n",
    "    for batch_examples in tqdm(loader, desc=f\"Evaluating {task}\"):\n",
    "        # Move tensor parts of batch to device\n",
    "        batch = {\n",
    "            k: v.to(model.device)\n",
    "            for k, v in batch_examples.items()\n",
    "            if isinstance(v, torch.Tensor)\n",
    "        }\n",
    "\n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            gen = model.generate(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch.get(\"attention_mask\", None),\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        # Decode full sequences (prompt + continuation)\n",
    "        generated = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
    "\n",
    "        prefix = eval_collator.answer_prefix  # \"The correct output is\"\n",
    "\n",
    "        for full_output, gold in zip(generated, batch_examples[\"label_texts\"]):\n",
    "            # Take part after last \"The correct output is\"\n",
    "            idx = full_output.rfind(prefix)\n",
    "            if idx != -1:\n",
    "                pred = full_output[idx + len(prefix):].strip()\n",
    "            else:\n",
    "                pred = full_output.strip()\n",
    "\n",
    "            preds.append(pred)\n",
    "            golds.append(gold)\n",
    "\n",
    "    # ---- compute metric ----\n",
    "    if len(golds) == 0:\n",
    "        raise ValueError(\"No gold labels collected; check that label_texts are in the batch.\")\n",
    "\n",
    "    if task == \"stsb\":\n",
    "        # STS-B: Pearson correlation\n",
    "        pred_vals = []\n",
    "        gold_vals = []\n",
    "\n",
    "        for pred_str, gold_str in zip(preds, golds):\n",
    "            gold_val = _extract_first_number(str(gold_str))\n",
    "            pred_val = _extract_first_number(str(pred_str))\n",
    "\n",
    "            if gold_val is None:\n",
    "                continue\n",
    "\n",
    "            # If prediction didn't produce a number, default to midpoint (2.5)\n",
    "            if pred_val is None:\n",
    "                pred_val = 2.5\n",
    "\n",
    "            pred_vals.append(pred_val)\n",
    "            gold_vals.append(gold_val)\n",
    "\n",
    "        if len(pred_vals) < 2:\n",
    "            pearson = 0.0\n",
    "        else:\n",
    "            pearson, _ = pearsonr(pred_vals, gold_vals)\n",
    "\n",
    "        print(f\"[STS-B] Pearson: {pearson:.4f} (n={len(pred_vals)})\")\n",
    "        acc = pearson  # Return Pearson as the metric\n",
    "    else:\n",
    "        # Classification tasks: compare normalized label strings\n",
    "        norm_preds = []\n",
    "        norm_golds = []\n",
    "        for p, g in zip(preds, golds):\n",
    "            # crude normalization: first token, lowercase, strip punctuation\n",
    "            p_norm = p.split()[0] if p else \"\"\n",
    "            p_norm = p_norm.strip(\" .,:;!?\").lower()\n",
    "\n",
    "            g_norm = str(g).split()[0] if g else \"\"\n",
    "            g_norm = g_norm.strip(\" .,:;!?\").lower()\n",
    "\n",
    "            norm_preds.append(p_norm)\n",
    "            norm_golds.append(g_norm)\n",
    "\n",
    "        correct = sum(p == g for p, g in zip(norm_preds, norm_golds))\n",
    "        acc = correct / len(norm_golds)\n",
    "        print(f\"[{task}] accuracy: {correct}/{len(norm_golds)} = {acc:.4f}\")\n",
    "\n",
    "    print(\"preds[:10]\", preds[:10])\n",
    "    print(\"golds[:10]\", golds[:10])\n",
    "    return acc, preds, golds\n",
    "\n",
    "results = {}\n",
    "\n",
    "\n",
    "\n",
    "for task in [ \"stsb\", \"sst2\", \"qnli\", \"qqp\", \"cola\", \"mrpc\",]:\n",
    "    split_name = f\"validation_{task}\"\n",
    "    print(f\"\\nEvaluating {split_name}...\")\n",
    "\n",
    "    acc, preds, golds = evaluate_glue(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        dataset=multi_glue[split_name],\n",
    "        eval_collator=eval_collator,\n",
    "        task=task,\n",
    "        batch_size=32,\n",
    "        max_new_tokens=10,\n",
    "    )\n",
    "\n",
    "    results[task] = acc\n",
    "\n",
    "print(\"\\n=== GLUE Eval Results ===\")\n",
    "for task, acc in results.items():\n",
    "    print(f\"{task}: {acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af065a4b-b282-4ae0-b97c-59a199adaf3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
