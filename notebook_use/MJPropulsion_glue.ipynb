{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45837763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90574c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Hugging Face and temp\n",
    "import os, pathlib\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "\n",
    "# ✅ Step 1: Log in to Hugging Face\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"multitask_textqa_benchmark\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "589253ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60e1eca133074831a995cefc22167569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# ✅ Step 2: Define model name\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# ✅ Step 3: Load configuration\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.hidden_dropout_prob = 0.0\n",
    "config.attention_probs_dropout_prob = 0.0\n",
    "# config.num_labels = 2  # Uncomment if doing classification\n",
    "\n",
    "# ✅ Step 4: Load tokenizer with legacy=True to avoid conversion error\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    legacy=True  # Suppresses the warning/error with tokenizer.model\n",
    ")\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, torch_dtype=torch.bfloat16\n",
    "    \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "686b048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MJLoRAFA import apply_monkeyjump\n",
    "\n",
    "\n",
    "blocks_spec = {\n",
    "    #\"SiglipEncoderLayer\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25],\n",
    "    \"LlamaDecoderLayer\":  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31],\n",
    "}\n",
    "linears = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\"]  # add 'up_proj','down_proj', etc. \"out_proj\", \"fc1\", \"fc2\", \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "\n",
    "model = apply_monkeyjump(\n",
    "    model,\n",
    "    blocks=blocks_spec,\n",
    "    shared_expert=[\"o_proj\", \"gate_proj\"],\n",
    "    linears=linears,\n",
    "    rank=2, alpha=5.0,\n",
    "    temperature=1.0,   # router T\n",
    "    ema_momentum=0.5,\n",
    "    top_k=1,\n",
    "    rep_mode=\"token\",\n",
    "    jitter_noise=0.1,\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d6bf0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preamble variants: [[128006, 78191, 128007, 271], [128006, 78191, 128007]]\n",
      "EOT ID: 128009, EOS ID: 128009\n",
      "Insert token ID: 128001\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GlueLlavaDataCollator:\n",
    "    \"\"\"\n",
    "    Text-only multitask collator for Llama 3 style processors on GLUE.\n",
    "    \"\"\"\n",
    "    tokenizer: Any\n",
    "    is_train: bool = True\n",
    "    pad_to_multiple_of: Optional[int] = 8\n",
    "    answer_prefix: str = \"The correct output is\"\n",
    "    debug: bool = False\n",
    "    force_left_padding: bool = True\n",
    "    insert_token_id: int = 128001  # Token to insert before eot_id\n",
    "\n",
    "    def __post_init__(self):\n",
    "        tok = self.tokenizer\n",
    "\n",
    "        if self.force_left_padding:\n",
    "            tok.padding_side = \"left\"\n",
    "\n",
    "        if tok.pad_token_id is None:\n",
    "            tok.pad_token = tok.eos_token\n",
    "\n",
    "        self.pad_id = tok.pad_token_id\n",
    "        self.eos_id = tok.eos_token_id\n",
    "\n",
    "        # Build preamble by getting actual special token IDs for Llama 3\n",
    "        try:\n",
    "            start_header_id = tok.convert_tokens_to_ids(\"<|start_header_id|>\")\n",
    "            end_header_id = tok.convert_tokens_to_ids(\"<|end_header_id|>\")\n",
    "            assistant_ids = tok.encode(\"assistant\", add_special_tokens=False)\n",
    "            newline_ids = tok.encode(\"\\n\\n\", add_special_tokens=False)\n",
    "            \n",
    "            base_preamble = [start_header_id] + assistant_ids + [end_header_id] + newline_ids\n",
    "            \n",
    "            self._preamble_variants = [\n",
    "                base_preamble,\n",
    "                [start_header_id] + assistant_ids + [end_header_id],\n",
    "            ]\n",
    "            \n",
    "            self.eot_id = tok.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            if self.debug:\n",
    "                print(f\"[warn] Could not build Llama 3 preamble: {e}\")\n",
    "            self._preamble_variants = []\n",
    "            self.eot_id = self.eos_id\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"Preamble variants: {self._preamble_variants}\")\n",
    "            print(f\"EOT ID: {self.eot_id}, EOS ID: {self.eos_id}\")\n",
    "            print(f\"Insert token ID: {self.insert_token_id}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _rfind_subseq(hay, needle) -> int:\n",
    "        if not needle or len(needle) > len(hay):\n",
    "            return -1\n",
    "        for s in range(len(hay) - len(needle), -1, -1):\n",
    "            if hay[s:s + len(needle)] == needle:\n",
    "                return s\n",
    "        return -1\n",
    "\n",
    "    def _find_assistant_start(self, ids) -> int:\n",
    "        for needle in self._preamble_variants:\n",
    "            pos = self._rfind_subseq(ids, needle)\n",
    "            if pos != -1:\n",
    "                return pos + len(needle)\n",
    "        return -1\n",
    "\n",
    "    def _first_eos_after(self, ids, start) -> int:\n",
    "        if start < 0:\n",
    "            return -1\n",
    "        for i in range(start, len(ids)):\n",
    "            if ids[i] == self.eot_id or ids[i] == self.eos_id:\n",
    "                return i\n",
    "        return len(ids)\n",
    "\n",
    "    def _insert_token_before_eot(\n",
    "        self, \n",
    "        input_ids: torch.Tensor, \n",
    "        attention_mask: Optional[torch.Tensor]\n",
    "    ) -> tuple:\n",
    "        \"\"\"\n",
    "        Insert self.insert_token_id before <|eot_id|> in each sequence.\n",
    "        Result: content<|end_of_text|><|eot_id|>\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        \n",
    "        if attention_mask is None:\n",
    "            attention_mask = (input_ids != self.pad_id).long()\n",
    "        \n",
    "        new_input_ids_list = []\n",
    "        new_attention_mask_list = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            ids = input_ids[i].tolist()\n",
    "            attn = attention_mask[i].tolist()\n",
    "            \n",
    "            # Find the last eot_id position in the valid (non-pad) region\n",
    "            eot_pos = -1\n",
    "            for j in range(len(ids) - 1, -1, -1):\n",
    "                if attn[j] == 1 and ids[j] == self.eot_id:\n",
    "                    eot_pos = j\n",
    "                    break\n",
    "            \n",
    "            if eot_pos != -1:\n",
    "                # Check if insert_token_id is already right before eot_id\n",
    "                if eot_pos > 0 and ids[eot_pos - 1] == self.insert_token_id:\n",
    "                    # Already in correct position\n",
    "                    new_input_ids_list.append(ids)\n",
    "                    new_attention_mask_list.append(attn)\n",
    "                else:\n",
    "                    # Insert token before eot_id\n",
    "                    new_ids = ids[:eot_pos] + [self.insert_token_id] + ids[eot_pos:]\n",
    "                    new_attn = attn[:eot_pos] + [1] + attn[eot_pos:]\n",
    "                    new_input_ids_list.append(new_ids)\n",
    "                    new_attention_mask_list.append(new_attn)\n",
    "            else:\n",
    "                # No eot_id found, append at end of valid tokens\n",
    "                last_valid_idx = -1\n",
    "                for j in range(len(ids) - 1, -1, -1):\n",
    "                    if attn[j] == 1:\n",
    "                        last_valid_idx = j\n",
    "                        break\n",
    "                \n",
    "                if last_valid_idx >= 0 and ids[last_valid_idx] != self.insert_token_id:\n",
    "                    ids.append(self.insert_token_id)\n",
    "                    attn.append(1)\n",
    "                \n",
    "                new_input_ids_list.append(ids)\n",
    "                new_attention_mask_list.append(attn)\n",
    "        \n",
    "        # Find max length and pad\n",
    "        max_len = max(len(ids) for ids in new_input_ids_list)\n",
    "        \n",
    "        if self.pad_to_multiple_of:\n",
    "            max_len = ((max_len + self.pad_to_multiple_of - 1) // self.pad_to_multiple_of) * self.pad_to_multiple_of\n",
    "        \n",
    "        # Left-pad all sequences to max_len\n",
    "        for i in range(batch_size):\n",
    "            pad_len = max_len - len(new_input_ids_list[i])\n",
    "            if pad_len > 0:\n",
    "                new_input_ids_list[i] = [self.pad_id] * pad_len + new_input_ids_list[i]\n",
    "                new_attention_mask_list[i] = [0] * pad_len + new_attention_mask_list[i]\n",
    "        \n",
    "        new_input_ids = torch.tensor(new_input_ids_list, dtype=input_ids.dtype)\n",
    "        new_attention_mask = torch.tensor(new_attention_mask_list, dtype=attention_mask.dtype)\n",
    "        \n",
    "        return new_input_ids, new_attention_mask\n",
    "\n",
    "    def __call__(self, examples: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        texts: List[str] = []\n",
    "        label_texts: List[str] = []\n",
    "\n",
    "        for ex in examples:\n",
    "            user_text = ex[\"question\"]\n",
    "            gold = ex.get(\"answer\", \"\")\n",
    "\n",
    "            if self.is_train:\n",
    "                assistant_text = f\"{self.answer_prefix} {gold}\"\n",
    "\n",
    "                conversation = [\n",
    "                    {\"role\": \"user\", \"content\": user_text},\n",
    "                    {\"role\": \"assistant\", \"content\": assistant_text},\n",
    "                ]\n",
    "\n",
    "                text = self.tokenizer.apply_chat_template(\n",
    "                    conversation,\n",
    "                    add_generation_prompt=False,\n",
    "                    tokenize=False,\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                conversation = [\n",
    "                    {\"role\": \"user\", \"content\": user_text},\n",
    "                ]\n",
    "\n",
    "                base = self.tokenizer.apply_chat_template(\n",
    "                    conversation,\n",
    "                    add_generation_prompt=True,\n",
    "                    tokenize=False,\n",
    "                )\n",
    "\n",
    "                text = base + self.answer_prefix\n",
    "\n",
    "            texts.append(text)\n",
    "            label_texts.append(gold)\n",
    "\n",
    "        batch = self.tokenizer(\n",
    "            text=texts,\n",
    "            padding=True,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attn = batch.get(\"attention_mask\", None)\n",
    "\n",
    "        # Insert token 128001 before <|eot_id|> in each sequence\n",
    "        input_ids, attn = self._insert_token_before_eot(input_ids, attn)\n",
    "        batch[\"input_ids\"] = input_ids\n",
    "        batch[\"attention_mask\"] = attn\n",
    "\n",
    "        labels = torch.full_like(input_ids, -100)\n",
    "\n",
    "        if self.is_train:\n",
    "            for i in range(input_ids.size(0)):\n",
    "                valid_pos = attn[i].nonzero(as_tuple=False).squeeze(-1)\n",
    "                compact_ids = input_ids[i, valid_pos].tolist()\n",
    "\n",
    "                start_c = self._find_assistant_start(compact_ids)\n",
    "                end_c = self._first_eos_after(compact_ids, start_c) if start_c != -1 else -1\n",
    "                end_c = end_c   # include EOS/eot_id in label span\n",
    "\n",
    "                if start_c != -1 and end_c > start_c:\n",
    "                    span_pos = valid_pos[start_c:end_c]\n",
    "                    labels[i, span_pos] = input_ids[i, span_pos]\n",
    "                elif self.debug:\n",
    "                    print(f\"[warn] assistant span not found for sample {i}\")\n",
    "                    print(f\"  compact_ids: {compact_ids[:50]}...\")\n",
    "                    print(f\"  looking for: {self._preamble_variants}\")\n",
    "        else:\n",
    "            batch[\"label_texts\"] = label_texts\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "train_collator = GlueLlavaDataCollator(tokenizer=tokenizer, is_train=True,  debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebf6b1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = train_collator(dataset['train'].select(range(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "324dd46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "To properly, thoroughly apply bug spray while camping,\n",
      "\n",
      "  Options: \n",
      " A. spray a heavy amount of the spray into the air, and then let the mist come down over you and coat you gently.\n",
      " B. stand with your arms wide apart and your mouth and eyes closed, while someone sprays you from top to bottom.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The correct output is B<|end_of_text|><|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(d['input_ids'][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d62eda2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!The correct output is B<|end_of_text|>!\n"
     ]
    }
   ],
   "source": [
    "k = []\n",
    "\n",
    "for i in d['labels'][2]:\n",
    "    if i == -100:\n",
    "        k.append(0)\n",
    "    else:\n",
    "        k.append(i)\n",
    "    \n",
    "print(tokenizer.decode(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "768e2356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'prompt', 'source', 'answer', 'category', 'dataset'],\n",
       "        num_rows: 70302\n",
       "    })\n",
       "    test_arc_challenge: Dataset({\n",
       "        features: ['question', 'prompt', 'source', 'answer', 'category', 'dataset'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test_arc_easy: Dataset({\n",
       "        features: ['question', 'prompt', 'source', 'answer', 'category', 'dataset'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test_boolq: Dataset({\n",
       "        features: ['question', 'prompt', 'source', 'answer', 'category', 'dataset'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_hellaswag: Dataset({\n",
       "        features: ['question', 'prompt', 'source', 'answer', 'category', 'dataset'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_openbookqa: Dataset({\n",
       "        features: ['question', 'prompt', 'source', 'answer', 'category', 'dataset'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test_piqa: Dataset({\n",
       "        features: ['question', 'prompt', 'source', 'answer', 'category', 'dataset'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_social_i_qa: Dataset({\n",
       "        features: ['question', 'prompt', 'source', 'answer', 'category', 'dataset'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_winogrande: Dataset({\n",
       "        features: ['question', 'prompt', 'source', 'answer', 'category', 'dataset'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73d43e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MonkeyTrainer] interval=1, stop_at=1000, update_on=micro, momentum=0.5\n"
     ]
    }
   ],
   "source": [
    "from MJtrainer import MonkeyTrainer\n",
    "from transformers import Trainer, TrainingArguments, TrainerCallback\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llava-lora-finetuned_our\",\n",
    "    per_device_train_batch_size=6,\n",
    "    gradient_accumulation_steps=2,  \n",
    "    save_total_limit=4,\n",
    "    save_steps=500000,\n",
    "    num_train_epochs=1,\n",
    "    remove_unused_columns=False, \n",
    "   \n",
    "    bf16=True,  \n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"no\",  \n",
    "    #eval_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "  \n",
    "    learning_rate=1e-3,\n",
    "    warmup_ratio=0.03,\n",
    "    weight_decay=0.00,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# moe_trainer.py\n",
    "import torch\n",
    "from transformers import Trainer\n",
    "\n",
    "\n",
    "# Example instantiation:\n",
    "trainer = MonkeyTrainer(\n",
    "    model=model,\n",
    "    args=training_args,                  # your HF TrainingArguments\n",
    "    train_dataset=dataset['train'],\n",
    "\n",
    "    data_collator=train_collator,         # your collator\n",
    "    momentum=0.5, \n",
    "\n",
    "\n",
    "    step_interval=1,                    # update every 10 train steps\n",
    "    stop_update_step=1000,               # stop updates at/after step 1000\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c2ba9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "[kmeans-init] Collecting representations (TOKEN-BASED)...\n",
      "============================================================\n",
      "[kmeans-init] Found 32 patched blocks, mode: TOKEN-BASED\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80045ebb427d4e609d1e09ebf59f4d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting representations:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "[kmeans-init] Block collected 10000 tokens, dim=4096\n",
      "\n",
      "============================================================\n",
      "[kmeans-init] Initializing router centers...\n",
      "============================================================\n",
      "[kmeans-init] model.layers.0: E=3 (avg_sim=0.408)\n",
      "[kmeans-init] model.layers.1: E=3 (avg_sim=0.526)\n",
      "[kmeans-init] model.layers.2: E=3 (avg_sim=0.492)\n",
      "[kmeans-init] model.layers.3: E=3 (avg_sim=0.489)\n",
      "[kmeans-init] model.layers.4: E=3 (avg_sim=0.509)\n",
      "[kmeans-init] model.layers.5: E=3 (avg_sim=0.502)\n",
      "[kmeans-init] model.layers.6: E=3 (avg_sim=0.494)\n",
      "[kmeans-init] model.layers.7: E=3 (avg_sim=0.482)\n",
      "[kmeans-init] model.layers.8: E=3 (avg_sim=0.494)\n",
      "[kmeans-init] model.layers.9: E=3 (avg_sim=0.515)\n",
      "[kmeans-init] model.layers.10: E=3 (avg_sim=0.500)\n",
      "[kmeans-init] model.layers.11: E=3 (avg_sim=0.517)\n",
      "[kmeans-init] model.layers.12: E=3 (avg_sim=0.516)\n",
      "[kmeans-init] model.layers.13: E=3 (avg_sim=0.575)\n",
      "[kmeans-init] model.layers.14: E=3 (avg_sim=0.558)\n",
      "[kmeans-init] model.layers.15: E=3 (avg_sim=0.559)\n",
      "[kmeans-init] model.layers.16: E=3 (avg_sim=0.565)\n",
      "[kmeans-init] model.layers.17: E=3 (avg_sim=0.585)\n",
      "[kmeans-init] model.layers.18: E=3 (avg_sim=0.591)\n",
      "[kmeans-init] model.layers.19: E=3 (avg_sim=0.609)\n",
      "[kmeans-init] model.layers.20: E=3 (avg_sim=0.619)\n",
      "[kmeans-init] model.layers.21: E=3 (avg_sim=0.612)\n",
      "[kmeans-init] model.layers.22: E=3 (avg_sim=0.619)\n",
      "[kmeans-init] model.layers.23: E=3 (avg_sim=0.622)\n",
      "[kmeans-init] model.layers.24: E=3 (avg_sim=0.631)\n",
      "[kmeans-init] model.layers.25: E=3 (avg_sim=0.643)\n",
      "[kmeans-init] model.layers.26: E=3 (avg_sim=0.661)\n",
      "[kmeans-init] model.layers.27: E=3 (avg_sim=0.667)\n",
      "[kmeans-init] model.layers.28: E=3 (avg_sim=0.673)\n",
      "[kmeans-init] model.layers.29: E=3 (avg_sim=0.674)\n",
      "[kmeans-init] model.layers.30: E=3 (avg_sim=0.644)\n",
      "[kmeans-init] model.layers.31: E=3 (avg_sim=0.641)\n",
      "\n",
      "============================================================\n",
      " INITIALIZATION SUMMARY\n",
      "============================================================\n",
      " Mode: TOKEN (clustering tokens)\n",
      "------------------------------------------------------------\n",
      " Block                                      E_init   E_used   AvgSim\n",
      "------------------------------------------------------------\n",
      "  model.layers.0                                 3        3    0.408\n",
      "  model.layers.1                                 3        3    0.526\n",
      "  model.layers.2                                 3        3    0.492\n",
      "  model.layers.3                                 3        3    0.489\n",
      "  model.layers.4                                 3        3    0.509\n",
      "  model.layers.5                                 3        3    0.502\n",
      "  model.layers.6                                 3        3    0.494\n",
      "  model.layers.7                                 3        3    0.482\n",
      "  model.layers.8                                 3        3    0.494\n",
      "  model.layers.9                                 3        3    0.515\n",
      "  model.layers.10                                3        3    0.500\n",
      "  model.layers.11                                3        3    0.517\n",
      "  model.layers.12                                3        3    0.516\n",
      "  model.layers.13                                3        3    0.575\n",
      "  model.layers.14                                3        3    0.558\n",
      "  model.layers.15                                3        3    0.559\n",
      "  model.layers.16                                3        3    0.565\n",
      "  model.layers.17                                3        3    0.585\n",
      "  model.layers.18                                3        3    0.591\n",
      "  model.layers.19                                3        3    0.609\n",
      "  model.layers.20                                3        3    0.619\n",
      "  model.layers.21                                3        3    0.612\n",
      "  model.layers.22                                3        3    0.619\n",
      "  model.layers.23                                3        3    0.622\n",
      "  model.layers.24                                3        3    0.631\n",
      "  model.layers.25                                3        3    0.643\n",
      "  model.layers.26                                3        3    0.661\n",
      "  model.layers.27                                3        3    0.667\n",
      "  model.layers.28                                3        3    0.673\n",
      "  model.layers.29                                3        3    0.674\n",
      "  model.layers.30                                3        3    0.644\n",
      "  model.layers.31                                3        3    0.641\n",
      "============================================================\n",
      "[kmeans-init] Done!\n"
     ]
    }
   ],
   "source": [
    "from kmneas import init_router_centers\n",
    "init_router_centers(\n",
    "    trainer,\n",
    "    subset_size=5000,        # Use 2000 samples\n",
    "    loader_batch_size=8,\n",
    "    collect_batches=2000,\n",
    "    kmeans_iters=30,\n",
    "    seed=42,\n",
    "    verbose=True,\n",
    "  \n",
    "    # Optional: auto-select number of experts\n",
    "    auto_select_experts=False,\n",
    "    rep_mode=\"token\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84318271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5858' max='5858' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5858/5858 37:52, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.783700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.170600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.154200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.148100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.155200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.136000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.134500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.137600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.123900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.133300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.126200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.123900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.127100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.118100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.116600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.119200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.132100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.119400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.126900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.109600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.117700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.122800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.115600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.115800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.114000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.109600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.110200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.109200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.102200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.106400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.102000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.092500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.105900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.103300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.114200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.103600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.103800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.104800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.102200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.091800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.086700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.097800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.091800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.098400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.095000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.099800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.088700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.100700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.106800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.080800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.089700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.099200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.095300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.095700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.101100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.082800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.092500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.086800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5858, training_loss=0.20806089258307928, metrics={'train_runtime': 2273.1626, 'train_samples_per_second': 30.927, 'train_steps_per_second': 2.577, 'total_flos': 3.897378324849623e+17, 'train_loss': 0.20806089258307928, 'epoch': 0.9999146539216524})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "931b6687",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_collator  = GlueLlavaDataCollator(tokenizer=tokenizer, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "342c2378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating test_winogrande...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating commonsense: 100%|██████████| 40/40 [00:24<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[commonsense] accuracy: 0/1267 = 0.0000\n",
      "preds[:10] ['maria', 'sarah', 'bed', 'B', 'B', 'sarah', 'the', 'B', 'jennifer', 'the']\n",
      "golds[:10] ['option2', 'option1', 'option2', 'option1', 'option1', 'option1', 'option1', 'option2', 'option2', 'option1']\n",
      "\n",
      "Evaluating test_arc_challenge...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating commonsense: 100%|██████████| 37/37 [00:14<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[commonsense] accuracy: 930/1172 = 0.7935\n",
      "preds[:10] ['C', 'B', 'C', 'C', 'D', 'B', 'C', 'C', 'C', 'A']\n",
      "golds[:10] ['C', 'B', 'C', 'D', 'D', 'B', 'C', 'C', 'B', 'A']\n",
      "\n",
      "Evaluating test_arc_easy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating commonsense: 100%|██████████| 75/75 [00:28<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[commonsense] accuracy: 2033/2376 = 0.8556\n",
      "preds[:10] ['A', 'B', 'D', 'B', 'C', 'C', 'B', 'C', 'C', 'A']\n",
      "golds[:10] ['A', 'B', 'D', 'D', 'B', 'C', 'A', 'C', 'C', 'A']\n",
      "\n",
      "Evaluating test_boolq...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating commonsense: 100%|██████████| 103/103 [00:21<00:00,  4.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[commonsense] accuracy: 2305/3270 = 0.7049\n",
      "preds[:10] ['A', 'A', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'A']\n",
      "golds[:10] ['B', 'A', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'A']\n",
      "\n",
      "Evaluating test_hellaswag...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating commonsense:  35%|███▌      | 110/314 [00:56<01:45,  1.94it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 121\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split_name \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mkeys():  \u001b[38;5;66;03m# or your datasetdict variable\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m     acc, preds, golds \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_commonsense\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m     results[split_name] \u001b[38;5;241m=\u001b[39m acc\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Commonsense Eval Results ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 77\u001b[0m, in \u001b[0;36mevaluate_commonsense\u001b[0;34m(model, tokenizer, dataset, eval_collator, batch_size, max_new_tokens)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m#print(processor.tokenizer.decode(batch[\"input_ids\"][0]))\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 77\u001b[0m     gen \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m generated \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(gen, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m#print(generated[0])\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m#break\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/transformers/generation/utils.py:2326\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2318\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2319\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2320\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2321\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2322\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2323\u001b[0m     )\n\u001b[1;32m   2325\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2326\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2331\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2333\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2334\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2336\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2337\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2338\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2339\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2340\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2341\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2342\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2343\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/transformers/generation/utils.py:3286\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3283\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[0;32m-> 3286\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3287\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3288\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/accelerate/utils/operations.py:820\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 820\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/accelerate/utils/operations.py:808\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 808\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/torch/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:853\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    850\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    852\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 853\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    867\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    868\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:601\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    589\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    590\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    591\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    598\u001b[0m         position_embeddings,\n\u001b[1;32m    599\u001b[0m     )\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 601\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/lustre/fs1/home/md096273/monkey/MJLoRAFA.py:741\u001b[0m, in \u001b[0;36m_patch_block_forward.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    738\u001b[0m     module\u001b[38;5;241m.\u001b[39mset_routing(top_k_ids, top_k_weights)\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 741\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43morig_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m block\u001b[38;5;241m.\u001b[39m_mj_routed_modules:\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:343\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    342\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 343\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    356\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:277\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    275\u001b[0m hidden_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[0;32m--> 277\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    278\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    279\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/lustre/fs1/home/md096273/monkey/MJLoRAFA.py:221\u001b[0m, in \u001b[0;36mMonkeyJumpLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    218\u001b[0m is_token_based \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_top_k_ids\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m  \u001b[38;5;66;03m# [B, T, k] vs [B, k]\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_token_based:\n\u001b[0;32m--> 221\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_token_selective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_sequence_selective(x, base)\n",
      "File \u001b[0;32m/lustre/fs1/home/md096273/monkey/MJLoRAFA.py:186\u001b[0m, in \u001b[0;36mMonkeyJumpLinear._forward_token_selective\u001b[0;34m(self, x, base)\u001b[0m\n\u001b[1;32m    183\u001b[0m mask \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_top_k_weights \u001b[38;5;241m*\u001b[39m expert_mask\u001b[38;5;241m.\u001b[39mfloat())\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [B, T]\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# If this expert is never selected for any token in the batch, skip LoRA\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m base\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# Dense compute: LoRA for all tokens\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "LETTER = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "def make_mcq_pattern(prefix: str):\n",
    "    return re.compile(\n",
    "        rf\"({prefix}[1-9])\\s*:\\s*(.*?)(?=\\s+{prefix}[1-9]\\s*:|\\s+Answer format:|$)\",\n",
    "        flags=re.IGNORECASE | re.DOTALL\n",
    "    )\n",
    "\n",
    "PATTERNS = [\n",
    "    (make_mcq_pattern(\"Answer\"),   lambda k: k.lower()),   # Answer1..Answer4 (ARC/OpenBookQA/SocialIQA)\n",
    "    (make_mcq_pattern(\"Ending\"),   lambda k: k.lower()),   # Ending1..Ending4 (HellaSwag)\n",
    "    (make_mcq_pattern(\"Solution\"), lambda k: k.lower()),   # Solution1..Solution2 (PIQA)\n",
    "    (make_mcq_pattern(\"Option\"),   lambda k: k.lower()),   # Option1..Option2 (Winogrande)\n",
    "]\n",
    "\n",
    "def extract_question_text(instr: str) -> str:\n",
    "    m = re.search(r\"question:\\s*(.*?)(?:\\n\\s*\\n|$)\", instr, flags=re.I | re.S)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "\n",
    "    m2 = re.search(r\"to the question:\\s*(.*?)(?:\\n\\s*\\n|$)\", instr, flags=re.I | re.S)\n",
    "    if m2:\n",
    "        return m2.group(1).strip()\n",
    "\n",
    "    m3 = re.search(r\"sentence:\\s*(.*?)(?:\\n\\s*\\n|$)\", instr, flags=re.I | re.S)\n",
    "    if m3:\n",
    "        return m3.group(1).strip()\n",
    "\n",
    "    return instr.strip()\n",
    "\n",
    "def extract_options(instr: str):\n",
    "    for pat, norm_key in PATTERNS:\n",
    "        found = pat.findall(instr)\n",
    "        if found:\n",
    "            return [(norm_key(k), v.strip()) for k, v in found]\n",
    "\n",
    "    if re.search(r\"Answer format:\\s*true\\s*/\\s*false\", instr, flags=re.I):\n",
    "        return [(\"true\", \"true\"), (\"false\", \"false\")]\n",
    "\n",
    "    return []\n",
    "\n",
    "def build_answer_map(option_keys):\n",
    "    return {k: LETTER[i] for i, k in enumerate(option_keys)}\n",
    "\n",
    "def instruction_to_train_style(example, *, category: str, dataset_name: str):\n",
    "    instr = (example.get(\"instruction\") or \"\").strip()\n",
    "    gold = (example.get(\"answer\") or \"\").strip().lower()\n",
    "\n",
    "    # ---- Winogrande special case ----\n",
    "    if dataset_name == \"winogrande\":\n",
    "        # Extract options from instruction text: \"Option1: Sarah Option2: Maria\"\n",
    "        opt_match = re.search(r\"Option1:\\s*(\\S+)\\s+Option2:\\s*(\\S+)\", instr, re.IGNORECASE)\n",
    "        \n",
    "        if opt_match:\n",
    "            opt1 = opt_match.group(1)\n",
    "            opt2 = opt_match.group(2)\n",
    "            \n",
    "            # Extract question (sentence with blank) - text before \"Option1:\"\n",
    "            parts = re.split(r\"\\s*Option1:\", instr, flags=re.IGNORECASE)\n",
    "            q_text = parts[0].strip()\n",
    "            \n",
    "            # Clean up the question text - remove the prompt prefix\n",
    "            q_text = re.sub(r\"^Please choose the correct answer to fill in the blank to complete the given sentence:\\s*\", \"\", q_text, flags=re.I)\n",
    "            \n",
    "            question = (\n",
    "                f\"{q_text}\\n\\n\"\n",
    "                f\"  Options: \\n\"\n",
    "                f\" A. {opt1}\\n\"\n",
    "                f\" B. {opt2}\"\n",
    "            )\n",
    "            \n",
    "            # Map option1 -> A, option2 -> B\n",
    "            answer = {\"option1\": \"A\", \"option2\": \"B\"}.get(gold, gold)\n",
    "            \n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"prompt\": \"Choose the correct answer to fill in the blank.\",\n",
    "                \"source\": \"\",\n",
    "                \"answer\": answer,\n",
    "                \"category\": category,\n",
    "                \"dataset\": dataset_name,\n",
    "            }\n",
    "        \n",
    "        # Fallback if pattern doesn't match\n",
    "        return {\n",
    "            \"question\": extract_question_text(instr),\n",
    "            \"prompt\": \"Choose the correct answer to fill in the blank.\",\n",
    "            \"source\": \"\",\n",
    "            \"answer\": gold,\n",
    "            \"category\": category,\n",
    "            \"dataset\": dataset_name,\n",
    "        }\n",
    "\n",
    "    # ---- Default path (all other datasets) ----\n",
    "    q_text = extract_question_text(instr)\n",
    "    options = extract_options(instr)\n",
    "\n",
    "    if options:\n",
    "        option_keys = [k for k, _ in options]\n",
    "        option_texts = [t for _, t in options]\n",
    "        ans_map = build_answer_map(option_keys)\n",
    "\n",
    "        question = q_text + \"\\n\\n  Options: \\n\"\n",
    "        for i, text in enumerate(option_texts):\n",
    "            question += f\" {LETTER[i]}. {text}\\n\"\n",
    "        question = question.rstrip()\n",
    "\n",
    "        answer = ans_map.get(gold, gold)\n",
    "    else:\n",
    "        question = q_text\n",
    "        answer = gold\n",
    "\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"prompt\": \"Choose the correct answer to the question.\",\n",
    "        \"source\": \"\",\n",
    "        \"answer\": answer,\n",
    "        \"category\": category,\n",
    "        \"dataset\": dataset_name,\n",
    "    }\n",
    "\n",
    "def load_and_convert(url_or_path: str, *, category: str, dataset_name: str):\n",
    "    ds = load_dataset(\"json\", data_files=url_or_path, split=\"train\")\n",
    "    ds2 = ds.map(\n",
    "        instruction_to_train_style,\n",
    "        fn_kwargs={\"category\": category, \"dataset_name\": dataset_name},\n",
    "        remove_columns=ds.column_names,\n",
    "        desc=f\"Converting {dataset_name}\",\n",
    "    )\n",
    "    return ds2\n",
    "\n",
    "TEST_SOURCES = {\n",
    "    \"test_winogrande\": {\n",
    "        \"path\": \"https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/refs/heads/main/dataset/winogrande/test.json\",\n",
    "        \"category\": \"WinoGrande\",\n",
    "        \"dataset\": \"winogrande\",\n",
    "    },\n",
    "    \"test_arc_challenge\": {\n",
    "        \"path\": \"https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/refs/heads/main/dataset/ARC-Challenge/test.json\",\n",
    "        \"category\": \"ARC-Challenge\",\n",
    "        \"dataset\": \"arc_challenge\",\n",
    "    },\n",
    "    \"test_arc_easy\": {\n",
    "        \"path\": \"https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/refs/heads/main/dataset/ARC-Easy/test.json\",\n",
    "        \"category\": \"ARC-Easy\",\n",
    "        \"dataset\": \"arc_easy\",\n",
    "    },\n",
    "    \"test_boolq\": {\n",
    "        \"path\": \"https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/refs/heads/main/dataset/boolq/test.json\",\n",
    "        \"category\": \"BoolQ\",\n",
    "        \"dataset\": \"boolq\",\n",
    "    },\n",
    "    \"test_hellaswag\": {\n",
    "        \"path\": \"https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/refs/heads/main/dataset/hellaswag/test.json\",\n",
    "        \"category\": \"HellaSwag\",\n",
    "        \"dataset\": \"hellaswag\",\n",
    "    },\n",
    "    \"test_openbookqa\": {\n",
    "        \"path\": \"https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/refs/heads/main/dataset/openbookqa/test.json\",\n",
    "        \"category\": \"OpenBookQA\",\n",
    "        \"dataset\": \"openbookqa\",\n",
    "    },\n",
    "    \"test_piqa\": {\n",
    "        \"path\": \"https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/refs/heads/main/dataset/piqa/test.json\",\n",
    "        \"category\": \"PIQA\",\n",
    "        \"dataset\": \"piqa\",\n",
    "    },\n",
    "    \"test_social_i_qa\": {\n",
    "        \"path\": \"https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/refs/heads/main/dataset/social_i_qa/test.json\",\n",
    "        \"category\": \"SocialIQA\",\n",
    "        \"dataset\": \"social_i_qa\",\n",
    "    },\n",
    "}\n",
    "\n",
    "updated_tests = {\n",
    "    split: load_and_convert(meta[\"path\"], category=meta[\"category\"], dataset_name=meta[\"dataset\"])\n",
    "    for split, meta in TEST_SOURCES.items()\n",
    "}\n",
    "\n",
    "dataset = DatasetDict(updated_tests)\n",
    "\n",
    "# Verify Winogrande is processed correctly\n",
    "print(\"=== Winogrande Sample ===\")\n",
    "print(dataset[\"test_winogrande\"][0])\n",
    "print(f\"\\nAnswer: {dataset['test_winogrande'][0]['answer']}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def _extract_choice(pred: str):\n",
    "    \"\"\"\n",
    "    Extract a discrete choice from model text.\n",
    "    Supports:\n",
    "      - A/B/C/D\n",
    "      - answer1/answer2/answer3/answer4\n",
    "      - ending1..ending4\n",
    "      - solution1/solution2\n",
    "      - option1/option2 (Winogrande)\n",
    "      - true/false\n",
    "    Returns normalized label like \"A\"/\"B\"/\"C\"/\"D\" or \"true\"/\"false\" or \"\".\n",
    "    \"\"\"\n",
    "    if pred is None:\n",
    "        return \"\"\n",
    "\n",
    "    s = str(pred).strip().lower()\n",
    "\n",
    "    # common wrappers\n",
    "    # e.g. \"the correct output is B\", \"the correct answer is answer3\"\n",
    "    s = re.sub(r\"^the correct (output|answer) is\\s+\", \"\", s).strip()\n",
    "\n",
    "    # If it directly contains a letter choice, prefer first standalone A-D\n",
    "    m = re.search(r\"\\b([abcd])\\b\", s)\n",
    "    if m:\n",
    "        return m.group(1).upper()\n",
    "\n",
    "    # Map answer/ending/solution/option tokens to letters (1->A, 2->B, 3->C, 4->D)\n",
    "    m = re.search(r\"\\b(answer|ending|solution|option)\\s*([1-4])\\b\", s)\n",
    "    if m:\n",
    "        idx = int(m.group(2)) - 1\n",
    "        return \"ABCD\"[idx]\n",
    "\n",
    "    # BoolQ / yes-no style\n",
    "    if re.search(r\"\\btrue\\b\", s):\n",
    "        return \"true\"\n",
    "    if re.search(r\"\\bfalse\\b\", s):\n",
    "        return \"false\"\n",
    "\n",
    "    # fallback: first token stripped\n",
    "    tok = s.split()[0] if s else \"\"\n",
    "    tok = tok.strip(\" .,:;!?\")\n",
    "    return tok\n",
    "\n",
    "def evaluate_commonsense(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    dataset,\n",
    "    eval_collator,\n",
    "    batch_size: int = 8,\n",
    "    max_new_tokens: int = 10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluation for commonsense reasoning DatasetDict splits.\n",
    "    Metric: exact-match accuracy after normalization.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    golds = []\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, collate_fn=eval_collator)\n",
    "\n",
    "    for batch_examples in tqdm(loader, desc=\"Evaluating commonsense\"):\n",
    "        batch = {\n",
    "            k: v.to(model.device)\n",
    "            for k, v in batch_examples.items()\n",
    "            if isinstance(v, torch.Tensor)\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            gen = model.generate(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch.get(\"attention_mask\", None),\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        generated = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
    "\n",
    "        prefix = eval_collator.answer_prefix  # e.g. \"The correct output is\"\n",
    "\n",
    "        for full_output, gold in zip(generated, batch_examples[\"label_texts\"]):\n",
    "            idx = full_output.rfind(prefix)\n",
    "            if idx != -1:\n",
    "                pred_text = full_output[idx + len(prefix):].strip()\n",
    "            else:\n",
    "                pred_text = full_output.strip()\n",
    "\n",
    "            preds.append(pred_text)\n",
    "            golds.append(gold)\n",
    "\n",
    "    if len(golds) == 0:\n",
    "        raise ValueError(\"No gold labels collected; check that label_texts are in the batch.\")\n",
    "\n",
    "    norm_preds = [_extract_choice(p) for p in preds]\n",
    "    norm_golds = [_extract_choice(g) for g in golds]\n",
    "\n",
    "    correct = sum(p == g for p, g in zip(norm_preds, norm_golds))\n",
    "    acc = correct / len(norm_golds)\n",
    "\n",
    "    print(f\"[commonsense] accuracy: {correct}/{len(norm_golds)} = {acc:.4f}\")\n",
    "    print(\"preds[:10]\", norm_preds[:10])\n",
    "    print(\"golds[:10]\", norm_golds[:10])\n",
    "\n",
    "    return acc, preds, golds\n",
    "\n",
    "\n",
    "# Run evaluation\n",
    "results = {}\n",
    "\n",
    "for split_name in dataset.keys():\n",
    "    print(f\"\\nEvaluating {split_name}...\")\n",
    "    acc, preds, golds = evaluate_commonsense(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        dataset=dataset[split_name],\n",
    "        eval_collator=eval_collator,\n",
    "        batch_size=32,\n",
    "        max_new_tokens=10,\n",
    "    )\n",
    "    results[split_name] = acc\n",
    "\n",
    "print(\"\\n=== Commonsense Eval Results ===\")\n",
    "for split, acc in results.items():\n",
    "    print(f\"{split}: {acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ea947e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Winogrande Sample ===\n",
      "{'answer': 'B', 'question': 'Sarah was a much better surgeon than Maria so _ always got the easier cases.\\n\\n  Options: \\n A. Sarah\\n B. Maria', 'prompt': 'Choose the correct answer to fill in the blank.', 'source': '', 'category': 'WinoGrande', 'dataset': 'winogrande'}\n",
      "\n",
      "Answer: B\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "LETTER = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "def make_mcq_pattern(prefix: str):\n",
    "    return re.compile(\n",
    "        rf\"({prefix}[1-9])\\s*:\\s*(.*?)(?=\\s+{prefix}[1-9]\\s*:|\\s+Answer format:|$)\",\n",
    "        flags=re.IGNORECASE | re.DOTALL\n",
    "    )\n",
    "\n",
    "PATTERNS = [\n",
    "    (make_mcq_pattern(\"Answer\"),   lambda k: k.lower()),   # Answer1..Answer4 (ARC/OpenBookQA/SocialIQA)\n",
    "    (make_mcq_pattern(\"Ending\"),   lambda k: k.lower()),   # Ending1..Ending4 (HellaSwag)\n",
    "    (make_mcq_pattern(\"Solution\"), lambda k: k.lower()),   # Solution1..Solution2 (PIQA)\n",
    "    (make_mcq_pattern(\"Option\"),   lambda k: k.lower()),   # Option1..Option2 (Winogrande)\n",
    "]\n",
    "\n",
    "def extract_question_text(instr: str) -> str:\n",
    "    m = re.search(r\"question:\\s*(.*?)(?:\\n\\s*\\n|$)\", instr, flags=re.I | re.S)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "\n",
    "    m2 = re.search(r\"to the question:\\s*(.*?)(?:\\n\\s*\\n|$)\", instr, flags=re.I | re.S)\n",
    "    if m2:\n",
    "        return m2.group(1).strip()\n",
    "\n",
    "    m3 = re.search(r\"sentence:\\s*(.*?)(?:\\n\\s*\\n|$)\", instr, flags=re.I | re.S)\n",
    "    if m3:\n",
    "        return m3.group(1).strip()\n",
    "\n",
    "    return instr.strip()\n",
    "\n",
    "def extract_options(instr: str):\n",
    "    for pat, norm_key in PATTERNS:\n",
    "        found = pat.findall(instr)\n",
    "        if found:\n",
    "            return [(norm_key(k), v.strip()) for k, v in found]\n",
    "\n",
    "    if re.search(r\"Answer format:\\s*true\\s*/\\s*false\", instr, flags=re.I):\n",
    "        return [(\"true\", \"true\"), (\"false\", \"false\")]\n",
    "\n",
    "    return []\n",
    "\n",
    "def build_answer_map(option_keys):\n",
    "    return {k: LETTER[i] for i, k in enumerate(option_keys)}\n",
    "\n",
    "def instruction_to_train_style(example, *, category: str, dataset_name: str):\n",
    "    instr = (example.get(\"instruction\") or \"\").strip()\n",
    "    gold = (example.get(\"answer\") or \"\").strip().lower()\n",
    "\n",
    "    # ---- Winogrande special case ----\n",
    "    if dataset_name == \"winogrande\":\n",
    "        # Extract options from instruction text: \"Option1: Sarah Option2: Maria\"\n",
    "        opt_match = re.search(r\"Option1:\\s*(\\S+)\\s+Option2:\\s*(\\S+)\", instr, re.IGNORECASE)\n",
    "        \n",
    "        if opt_match:\n",
    "            opt1 = opt_match.group(1)\n",
    "            opt2 = opt_match.group(2)\n",
    "            \n",
    "            # Extract question (sentence with blank) - text before \"Option1:\"\n",
    "            parts = re.split(r\"\\s*Option1:\", instr, flags=re.IGNORECASE)\n",
    "            q_text = parts[0].strip()\n",
    "            \n",
    "            # Clean up the question text - remove the prompt prefix\n",
    "            q_text = re.sub(r\"^Please choose the correct answer to fill in the blank to complete the given sentence:\\s*\", \"\", q_text, flags=re.I)\n",
    "            \n",
    "            question = (\n",
    "                f\"{q_text}\\n\\n\"\n",
    "                f\"  Options: \\n\"\n",
    "                f\" A. {opt1}\\n\"\n",
    "                f\" B. {opt2}\"\n",
    "            )\n",
    "            \n",
    "            # Map option1 -> A, option2 -> B\n",
    "            answer = {\"option1\": \"A\", \"option2\": \"B\"}.get(gold, gold)\n",
    "            \n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"prompt\": \"Choose the correct answer to fill in the blank.\",\n",
    "                \"source\": \"\",\n",
    "                \"answer\": answer,\n",
    "                \"category\": category,\n",
    "                \"dataset\": dataset_name,\n",
    "            }\n",
    "        \n",
    "        # Fallback if pattern doesn't match\n",
    "        return {\n",
    "            \"question\": extract_question_text(instr),\n",
    "            \"prompt\": \"Choose the correct answer to fill in the blank.\",\n",
    "            \"source\": \"\",\n",
    "            \"answer\": gold,\n",
    "            \"category\": category,\n",
    "            \"dataset\": dataset_name,\n",
    "        }\n",
    "\n",
    "    # ---- Default path (all other datasets) ----\n",
    "    q_text = extract_question_text(instr)\n",
    "    options = extract_options(instr)\n",
    "\n",
    "    if options:\n",
    "        option_keys = [k for k, _ in options]\n",
    "        option_texts = [t for _, t in options]\n",
    "        ans_map = build_answer_map(option_keys)\n",
    "\n",
    "        question = q_text + \"\\n\\n  Options: \\n\"\n",
    "        for i, text in enumerate(option_texts):\n",
    "            question += f\" {LETTER[i]}. {text}\\n\"\n",
    "        question = question.rstrip()\n",
    "\n",
    "        answer = ans_map.get(gold, gold)\n",
    "    else:\n",
    "        question = q_text\n",
    "        answer = gold\n",
    "\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"prompt\": \"Choose the correct answer to the question.\",\n",
    "        \"source\": \"\",\n",
    "        \"answer\": answer,\n",
    "        \"category\": category,\n",
    "        \"dataset\": dataset_name,\n",
    "    }\n",
    "\n",
    "def load_and_convert(url_or_path: str, *, category: str, dataset_name: str):\n",
    "    ds = load_dataset(\"json\", data_files=url_or_path, split=\"train\")\n",
    "    ds2 = ds.map(\n",
    "        instruction_to_train_style,\n",
    "        fn_kwargs={\"category\": category, \"dataset_name\": dataset_name},\n",
    "        remove_columns=ds.column_names,\n",
    "        desc=f\"Converting {dataset_name}\",\n",
    "    )\n",
    "    return ds2\n",
    "\n",
    "TEST_SOURCES = {\n",
    "    \"test_winogrande\": {\n",
    "        \"path\": \"https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/refs/heads/main/dataset/winogrande/test.json\",\n",
    "        \"category\": \"WinoGrande\",\n",
    "        \"dataset\": \"winogrande\",\n",
    "    },\n",
    "    \"test_arc_challenge\": {\n",
    "        \"path\": \"https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/refs/heads/main/dataset/ARC-Challenge/test.json\",\n",
    "        \"category\": \"ARC-Challenge\",\n",
    "        \"dataset\": \"arc_challenge\",\n",
    "    },\n",
    "    \"test_arc_easy\": {\n",
    "        \"path\": \"https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/refs/heads/main/dataset/ARC-Easy/test.json\",\n",
    "        \"category\": \"ARC-Easy\",\n",
    "        \"dataset\": \"arc_easy\",\n",
    "    },\n",
    "    \"test_boolq\": {\n",
    "        \"path\": \"https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/refs/heads/main/dataset/boolq/test.json\",\n",
    "        \"category\": \"BoolQ\",\n",
    "        \"dataset\": \"boolq\",\n",
    "    },\n",
    "    \"test_hellaswag\": {\n",
    "        \"path\": \"https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/refs/heads/main/dataset/hellaswag/test.json\",\n",
    "        \"category\": \"HellaSwag\",\n",
    "        \"dataset\": \"hellaswag\",\n",
    "    },\n",
    "    \"test_openbookqa\": {\n",
    "        \"path\": \"https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/refs/heads/main/dataset/openbookqa/test.json\",\n",
    "        \"category\": \"OpenBookQA\",\n",
    "        \"dataset\": \"openbookqa\",\n",
    "    },\n",
    "    \"test_piqa\": {\n",
    "        \"path\": \"https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/refs/heads/main/dataset/piqa/test.json\",\n",
    "        \"category\": \"PIQA\",\n",
    "        \"dataset\": \"piqa\",\n",
    "    },\n",
    "    \"test_social_i_qa\": {\n",
    "        \"path\": \"https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/refs/heads/main/dataset/social_i_qa/test.json\",\n",
    "        \"category\": \"SocialIQA\",\n",
    "        \"dataset\": \"social_i_qa\",\n",
    "    },\n",
    "}\n",
    "\n",
    "updated_tests = {\n",
    "    split: load_and_convert(meta[\"path\"], category=meta[\"category\"], dataset_name=meta[\"dataset\"])\n",
    "    for split, meta in TEST_SOURCES.items()\n",
    "}\n",
    "\n",
    "dataset = DatasetDict(updated_tests)\n",
    "\n",
    "# Verify Winogrande is processed correctly\n",
    "print(\"=== Winogrande Sample ===\")\n",
    "print(dataset[\"test_winogrande\"][0])\n",
    "print(f\"\\nAnswer: {dataset['test_winogrande'][0]['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffcdd5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating test_winogrande...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating commonsense: 100%|██████████| 40/40 [00:11<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[commonsense] accuracy: 971/1267 = 0.7664\n",
      "preds[:10] ['B', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'A']\n",
      "golds[:10] ['B', 'A', 'B', 'A', 'A', 'A', 'A', 'B', 'B', 'A']\n",
      "\n",
      "Evaluating test_arc_challenge...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating commonsense: 100%|██████████| 37/37 [00:14<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[commonsense] accuracy: 931/1172 = 0.7944\n",
      "preds[:10] ['C', 'B', 'C', 'C', 'D', 'B', 'C', 'C', 'C', 'A']\n",
      "golds[:10] ['C', 'B', 'C', 'D', 'D', 'B', 'C', 'C', 'B', 'A']\n",
      "\n",
      "Evaluating test_arc_easy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating commonsense: 100%|██████████| 75/75 [00:27<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[commonsense] accuracy: 2011/2376 = 0.8464\n",
      "preds[:10] ['A', 'B', 'D', 'B', 'C', 'C', 'B', 'C', 'C', 'A']\n",
      "golds[:10] ['A', 'B', 'D', 'D', 'B', 'C', 'A', 'C', 'C', 'A']\n",
      "\n",
      "Evaluating test_boolq...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating commonsense: 100%|██████████| 103/103 [00:31<00:00,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[commonsense] accuracy: 2300/3270 = 0.7034\n",
      "preds[:10] ['A', 'B', 'B', 'A', 'A', 'B', 'A', 'A', 'A', 'A']\n",
      "golds[:10] ['B', 'A', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'A']\n",
      "\n",
      "Evaluating test_hellaswag...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating commonsense: 100%|██████████| 314/314 [03:54<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[commonsense] accuracy: 6312/10042 = 0.6286\n",
      "preds[:10] ['B', 'D', 'C', 'C', 'B', 'B', 'C', 'A', 'B', 'C']\n",
      "golds[:10] ['D', 'D', 'C', 'C', 'B', 'B', 'C', 'A', 'B', 'B']\n",
      "\n",
      "Evaluating test_openbookqa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating commonsense: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[commonsense] accuracy: 375/500 = 0.7500\n",
      "preds[:10] ['B', 'A', 'C', 'C', 'B', 'C', 'C', 'B', 'D', 'B']\n",
      "golds[:10] ['B', 'A', 'C', 'C', 'C', 'C', 'C', 'B', 'D', 'B']\n",
      "\n",
      "Evaluating test_piqa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating commonsense: 100%|██████████| 58/58 [00:33<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[commonsense] accuracy: 1451/1838 = 0.7894\n",
      "preds[:10] ['B', 'B', 'A', 'B', 'A', 'B', 'B', 'B', 'B', 'A']\n",
      "golds[:10] ['A', 'B', 'B', 'B', 'A', 'B', 'B', 'A', 'A', 'A']\n",
      "\n",
      "Evaluating test_social_i_qa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating commonsense: 100%|██████████| 62/62 [00:17<00:00,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[commonsense] accuracy: 1270/1954 = 0.6499\n",
      "preds[:10] ['A', 'B', 'B', 'A', 'C', 'B', 'A', 'B', 'C', 'B']\n",
      "golds[:10] ['C', 'A', 'B', 'A', 'C', 'A', 'B', 'B', 'C', 'B']\n",
      "\n",
      "=== Commonsense Eval Results ===\n",
      "test_winogrande: 0.7664\n",
      "test_arc_challenge: 0.7944\n",
      "test_arc_easy: 0.8464\n",
      "test_boolq: 0.7034\n",
      "test_hellaswag: 0.6286\n",
      "test_openbookqa: 0.7500\n",
      "test_piqa: 0.7894\n",
      "test_social_i_qa: 0.6499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def _extract_choice(pred: str):\n",
    "    \"\"\"\n",
    "    Extract a discrete choice from model text.\n",
    "    Supports:\n",
    "      - A/B/C/D\n",
    "      - answer1/answer2/answer3/answer4\n",
    "      - ending1..ending4\n",
    "      - solution1/solution2\n",
    "      - option1/option2 (Winogrande)\n",
    "      - true/false\n",
    "    Returns normalized label like \"A\"/\"B\"/\"C\"/\"D\" or \"true\"/\"false\" or \"\".\n",
    "    \"\"\"\n",
    "    if pred is None:\n",
    "        return \"\"\n",
    "\n",
    "    s = str(pred).strip().lower()\n",
    "\n",
    "    # common wrappers\n",
    "    # e.g. \"the correct output is B\", \"the correct answer is answer3\"\n",
    "    s = re.sub(r\"^the correct (output|answer) is\\s+\", \"\", s).strip()\n",
    "\n",
    "    # If it directly contains a letter choice, prefer first standalone A-D\n",
    "    m = re.search(r\"\\b([abcd])\\b\", s)\n",
    "    if m:\n",
    "        return m.group(1).upper()\n",
    "\n",
    "    # Map answer/ending/solution/option tokens to letters (1->A, 2->B, 3->C, 4->D)\n",
    "    m = re.search(r\"\\b(answer|ending|solution|option)\\s*([1-4])\\b\", s)\n",
    "    if m:\n",
    "        idx = int(m.group(2)) - 1\n",
    "        return \"ABCD\"[idx]\n",
    "\n",
    "    # BoolQ / yes-no style\n",
    "    if re.search(r\"\\btrue\\b\", s):\n",
    "        return \"true\"\n",
    "    if re.search(r\"\\bfalse\\b\", s):\n",
    "        return \"false\"\n",
    "\n",
    "    # fallback: first token stripped\n",
    "    tok = s.split()[0] if s else \"\"\n",
    "    tok = tok.strip(\" .,:;!?\")\n",
    "    return tok\n",
    "\n",
    "def evaluate_commonsense(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    dataset,\n",
    "    eval_collator,\n",
    "    batch_size: int = 8,\n",
    "    max_new_tokens: int = 10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluation for commonsense reasoning DatasetDict splits.\n",
    "    Metric: exact-match accuracy after normalization.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    golds = []\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, collate_fn=eval_collator)\n",
    "\n",
    "    for batch_examples in tqdm(loader, desc=\"Evaluating commonsense\"):\n",
    "        batch = {\n",
    "            k: v.to(model.device)\n",
    "            for k, v in batch_examples.items()\n",
    "            if isinstance(v, torch.Tensor)\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            gen = model.generate(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch.get(\"attention_mask\", None),\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        generated = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
    "\n",
    "        prefix = eval_collator.answer_prefix  # e.g. \"The correct output is\"\n",
    "\n",
    "        for full_output, gold in zip(generated, batch_examples[\"label_texts\"]):\n",
    "            idx = full_output.rfind(prefix)\n",
    "            if idx != -1:\n",
    "                pred_text = full_output[idx + len(prefix):].strip()\n",
    "            else:\n",
    "                pred_text = full_output.strip()\n",
    "\n",
    "            preds.append(pred_text)\n",
    "            golds.append(gold)\n",
    "\n",
    "    if len(golds) == 0:\n",
    "        raise ValueError(\"No gold labels collected; check that label_texts are in the batch.\")\n",
    "\n",
    "    norm_preds = [_extract_choice(p) for p in preds]\n",
    "    norm_golds = [_extract_choice(g) for g in golds]\n",
    "\n",
    "    correct = sum(p == g for p, g in zip(norm_preds, norm_golds))\n",
    "    acc = correct / len(norm_golds)\n",
    "\n",
    "    print(f\"[commonsense] accuracy: {correct}/{len(norm_golds)} = {acc:.4f}\")\n",
    "    print(\"preds[:10]\", norm_preds[:10])\n",
    "    print(\"golds[:10]\", norm_golds[:10])\n",
    "\n",
    "    return acc, preds, golds\n",
    "\n",
    "\n",
    "# Run evaluation\n",
    "results = {}\n",
    "\n",
    "for split_name in dataset.keys():\n",
    "    print(f\"\\nEvaluating {split_name}...\")\n",
    "    acc, preds, golds = evaluate_commonsense(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        dataset=dataset[split_name],\n",
    "        eval_collator=eval_collator,\n",
    "        batch_size=32,\n",
    "        max_new_tokens=10,\n",
    "    )\n",
    "    results[split_name] = acc\n",
    "\n",
    "print(\"\\n=== Commonsense Eval Results ===\")\n",
    "for split, acc in results.items():\n",
    "    print(f\"{split}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3921a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
