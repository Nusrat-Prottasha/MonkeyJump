<p align="center">
  <img src="assets/Money_Jump.png" alt="Monkey Jump Logo" width="400"/>
</p>

<h1 align="center"> Monkey Jump : MoE-Style PEFT for Efficient Multi-Task Learning</h1>
<p align="center">
  <a href="#" target="_blank">
    <img src="assets/web.png" alt="Project Website" height="36">
  </a>
  &nbsp;&nbsp;&nbsp;&nbsp;
  <a href="https://github.com/Nusrat-Prottasha/MonkeyJump" target="_blank">
    <img src="assets/github.png" alt="GitHub" height="36">
  </a>
  &nbsp;&nbsp;&nbsp;&nbsp;
  <a href="#" target="_blank">
    <img src="assets/arxiv.png" alt="arXiv Paper" height="36">
  </a>
</p>

---
<p align="center">
  <img src="assets/method.png" alt="Monkey Jump Logo" width="900"/>
</p>
## ğŸ”· Overview

Monkey Jump (MJ) achieves **MoE-style specialization** in parameter-efficient fine-tuning (PEFT) without adding trainable parameters. It treats existing PEFT adapters as **implicit experts** and routes tokens among them using **gradient-free k-means clustering** with **EMA-updated centers**.

### ğŸ¯ Key Innovation
> **Zero routing parameters** â€” Unlike traditional MoE-PEFT methods that require separate routing networks, Monkey Jump uses k-means clustering to dynamically assign tokens to experts, achieving specialization with zero routing overhead.

---

## âœ¨ Key Features

- ğŸ¯ **Zero additional trainable parameters** for routing
- ğŸ“‰ **7â€“29Ã— fewer parameters** than MoE-PEFT methods
- ğŸ’¾ **Up to 48% lower peak GPU memory** usage
- âš¡ **1.5â€“2Ã— faster** training and inference
- ğŸ”§ Supports multiple PEFT variants: **LoRA, LoRA-FA, AdaLoRA, Propulsion**
- ğŸ² Token-wise and sentence-wise routing modes

<p align="center">
  <img src="assets/method_overview.png" width="820" alt="Monkey Jump Method Overview"/>
</p>

---
## ğŸ”§ Requirements

### Software Dependencies

| Package | Version |
|---------|---------|
| Python | â‰¥ 3.10 |
| PyTorch | â‰¥ 2.1.0 |
| transformers | â‰¥ 4.50.0 |
| accelerate | â‰¥ 0.27.0 |
| datasets | â‰¥ 2.18.0 |
| peft | â‰¥ 0.10.0 |
| scikit-learn | â‰¥ 1.4.0 |
| LaTeX | For paper compilation |

### Hardware

All experiments were conducted on **NVIDIA H100 80GB GPUs** with CUDA 12.1+.

---

## âš™ï¸ Installation
```bash
git clone https://github.com/yourusername/MonkeyJump.git
cd MonkeyJump
pip install torch torchvision torchaudio
pip install transformers accelerate datasets peft
pip install scikit-learn tqdm numpy pandas
```

---

## ğŸš€ Usage

### Step 1: Apply MonkeyJump to Your Model
```python
from transformers import AutoModelForCausalLM
from src.MJLoRA import apply_monkeyjump

# Load your base model
model = AutoModelForCausalLM.from_pretrained("model_name")

# Apply MonkeyJump
model = apply_monkeyjump(
    model,
    blocks={"LlamaDecoderLayer": list(range(32))},
    linears=["q_proj", "k_proj", "v_proj", "o_proj"],
    shared_expert=["up_proj", "down_proj"],
    rank=2,
    alpha=16.0,
    temperature=1.0,
    ema_momentum=0.2,
    top_k=2,
    rep_mode="token",
)
```

### Step 2: Initialize Router Centers
```python
from src.kmneas import init_router_centers

init_router_centers(
    trainer,
    subset_size=4000,
    kmeans_iters=15,
    rep_mode="token",
)
```

### Step 3: Train with MonkeyTrainer
```python
from src.MJtrainer import MonkeyTrainer

trainer = MonkeyTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    step_interval=2,
    stop_update_step=3000,
)
trainer.train()
```

---

## ğŸ“Š Configuration Parameters

### MonkeyJump-LoRA Parameters

| Parameter | Description | Default |
|-----------|-------------|---------|
| `rank` | LoRA rank | 2 |
| `alpha` | LoRA scaling factor | 16.0 |
| `temperature` | Router softmax temperature | 1.0 |
| `ema_momentum` | EMA momentum for center updates | 0.9 |
| `top_k` | Number of experts to activate | 2 |
| `rep_mode` | Routing mode: token, last, mean | token |
| `jitter_noise` | Exploration noise (training) | 0.1 |

### Routing Modes

| Mode | Description | Output Shape |
|------|-------------|--------------|
| `token` | Per-token routing (default) | [B, T, k] |
| `last` | Last token representation | [B, k] |
| `mean` | Mean pooled representation | [B, k] |
| `prompt_end` | Token at prompt boundary | [B, k] |

---

## ğŸ§© Core Components

### 1ï¸âƒ£ MonkeyJumpLinear (MJLoRA.py)

Main implementation containing:
- **MonkeyJumpLinear**: LoRA adapter with routing mask
- **MonkeyJumpRouter**: Cosine-similarity router with EMA centers
- **apply_monkeyjump()**: Function to apply MJ to a model

### 2ï¸âƒ£ K-means Initialization (kmneas.py)

Utilities for initializing router centers via k-means++ with cosine similarity.

**Key functions:**
- `kmeans_pp_cosine()`: K-means++ initialization with cosine distance
- `collect_representations()`: Collect hidden representations from data
- `init_router_centers()`: Initialize all router centers in the model

### 3ï¸âƒ£ MonkeyTrainer (MJtrainer.py)

Custom HuggingFace Trainer with:
- âœ… Scheduled EMA center updates
- âœ… Configurable update intervals
- âœ… Automatic stopping of updates after N steps

---

## ğŸ“ˆ Performance & Efficiency

<p align="center">
  <img src="assets/efficiency_comparison.png" width="780" alt="Efficiency comparison"/>
</p>

### Key Metrics

| Metric | Improvement vs MoE-PEFT |
|--------|-------------------------|
| **Trainable Parameters** | 7-29Ã— fewer |
| **Peak GPU Memory** | Up to 48% lower |
| **Training Speed** | 1.5-2Ã— faster |
| **Inference Speed** | 1.5-2Ã— faster |

---

## ğŸ¯ Tips & Best Practices

### Getting Started
- Start with **`rank=2`** and **`top_k=2`** for most tasks
- Use **token-wise routing** (`rep_mode="token"`) as default
- Initialize with **4000-8000 samples** for stable clustering

### Optimization Strategies

#### ğŸ”¸ Reduce Memory Usage
- Lower `top_k` (fewer active experts)
- Use sentence-wise routing (`rep_mode="mean"` or `"last"`)
- Reduce `rank` if model is overfitting

#### ğŸ”¸ Improve Accuracy
- Increase `rank` (4-8 for complex tasks)
- Increase `top_k` (3-4 for better coverage)
- Use more k-means iterations during initialization

#### ğŸ”¸ Speed Up Training
- Stop EMA updates earlier (`stop_update_step=2000`)
- Use larger `step_interval` for EMA updates
- Reduce `subset_size` during initialization

### Hyperparameter Tuning Guide

| Issue | Solution |
|-------|----------|
| Underfitting | Increase `rank`, `top_k`, or `alpha` |
| Overfitting | Decrease `rank`, add regularization |
| Unstable training | Increase `ema_momentum`, lower learning rate |
| Routing too sharp | Increase `temperature` (e.g., 1.5-2.0) |
| Routing too soft | Decrease `temperature` (e.g., 0.5-0.8) |

---

## ğŸ“š Citation

If you use Monkey Jump in your research, please cite our paper:
```bibtex
@article{monkeyjump2024,
  title={Monkey Jump: MoE-Style Specialization for Parameter-Efficient Fine-Tuning},
  author={Your Name and Co-authors},
  journal={arXiv preprint arXiv:XXXX.XXXXX},
  year={2024}
}
```

**Under Review at [Conference Name] 2025**

---

## ğŸ¤ Contributing

We welcome contributions! Here's how you can help:

- ğŸ› Report bugs via [GitHub Issues](https://github.com/yourusername/MonkeyJump/issues)
- ğŸ’¡ Suggest features or improvements
- ğŸ“ Improve documentation
- ğŸ”§ Submit pull requests

Please see [CONTRIBUTING.md](CONTRIBUTING.md) for detailed guidelines.

---

## ğŸ“ License

Monkey Jump is licensed under the MIT License. See [LICENSE](LICENSE) file for details.

---

## ğŸ“§ Contact

- **Issues**: [GitHub Issues](https://github.com/yourusername/MonkeyJump/issues)
- **Email**: your.email@institution.edu
- **Project Website**: [Coming Soon](#)

---

## ğŸ™ Acknowledgments

- Built on top of [PEFT](https://github.com/huggingface/peft) and [Transformers](https://github.com/huggingface/transformers)
- Inspired by recent advances in MoE and parameter-efficient fine-tuning
- Thanks to all contributors and the open-source community

---

Â© 2024 Your Institution | Monkey Jump | MoE-PEFT without Routing Parameters
# Navigate to your repository
cd /path/to/MonkeyJump

# Create README.md (then paste the content)
nano README.md
# or
vim README.md
